{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd6e7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d54065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/input.txt\") as f:\n",
    "    text = f.read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddced999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f026a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57fa6de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47]\n",
      "['h', 'i', 'i']\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: [\"\".join(itos[i]) for i in l]\n",
    "\n",
    "print(encode(\"hii\"))\n",
    "print(decode(encode(\"hii\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce387c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]),\n",
       " tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "         53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "          1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "         57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "          6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "         58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "          1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "         53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "         57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "          8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "          1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "         53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "         47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "          1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "         50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "         49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "         47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
       "         46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
       "         43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
       "         54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
       "         47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
       "          1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
       "          1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
       "          1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
       "         47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
       "         53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
       "         58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
       "         39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
       "         47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
       "         39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
       "         46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
       "          1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
       "          1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
       "         50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
       "         56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
       "         61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
       "          1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
       "         56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
       "         50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
       "          1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
       "         58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
       "         39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
       "         40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
       "         63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
       "         53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
       "         57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
       "         11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
       "         57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
       "         43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
       "          1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
       "         56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
       "         10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
       "         61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
       "         46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
       "         52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
       "         56, 43, 60, 43, 52, 45, 43,  8,  0,  0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data.shape, data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3f3efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "data_train = data[:n]\n",
    "data_val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d23123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "print(data_train[:block_size+1])\n",
    "decode(data_train[:block_size+1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00f93a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18]) --> 47\n",
      "tensor([18, 47]) --> 56\n",
      "tensor([18, 47, 56]) --> 57\n",
      "tensor([18, 47, 56, 57]) --> 58\n",
      "tensor([18, 47, 56, 57, 58]) --> 1\n",
      "tensor([18, 47, 56, 57, 58,  1]) --> 15\n",
      "tensor([18, 47, 56, 57, 58,  1, 15]) --> 47\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) --> 58\n"
     ]
    }
   ],
   "source": [
    "x = data_train[:block_size]\n",
    "y = data_train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "\n",
    "    print(f\"{context} --> {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e941171a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data =  data_train if split == \"train\" else data_val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i  : i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb,yb = get_batch(\"train\")\n",
    "\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48c6804c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[43, 52,  1, 21,  1, 51, 43, 58],\n",
       "         [58,  1, 40, 63,  1, 39, 45, 43],\n",
       "         [50, 57,  0, 39, 50, 50,  1, 51],\n",
       "         [56, 53, 58, 46, 43, 56, 10,  1]]),\n",
       " tensor([[52,  1, 21,  1, 51, 43, 58,  1],\n",
       "         [ 1, 40, 63,  1, 39, 45, 43,  6],\n",
       "         [57,  0, 39, 50, 50,  1, 51, 43],\n",
       "         [53, 58, 46, 43, 56, 10,  1, 57]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67fca03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43] --> 52\n",
      "[43, 52] --> 1\n",
      "[43, 52, 1] --> 21\n",
      "[43, 52, 1, 21] --> 1\n",
      "[43, 52, 1, 21, 1] --> 51\n",
      "[43, 52, 1, 21, 1, 51] --> 43\n",
      "[43, 52, 1, 21, 1, 51, 43] --> 58\n",
      "[43, 52, 1, 21, 1, 51, 43, 58] --> 1\n",
      "[58] --> 1\n",
      "[58, 1] --> 40\n",
      "[58, 1, 40] --> 63\n",
      "[58, 1, 40, 63] --> 1\n",
      "[58, 1, 40, 63, 1] --> 39\n",
      "[58, 1, 40, 63, 1, 39] --> 45\n",
      "[58, 1, 40, 63, 1, 39, 45] --> 43\n",
      "[58, 1, 40, 63, 1, 39, 45, 43] --> 6\n",
      "[50] --> 57\n",
      "[50, 57] --> 0\n",
      "[50, 57, 0] --> 39\n",
      "[50, 57, 0, 39] --> 50\n",
      "[50, 57, 0, 39, 50] --> 50\n",
      "[50, 57, 0, 39, 50, 50] --> 1\n",
      "[50, 57, 0, 39, 50, 50, 1] --> 51\n",
      "[50, 57, 0, 39, 50, 50, 1, 51] --> 43\n",
      "[56] --> 53\n",
      "[56, 53] --> 58\n",
      "[56, 53, 58] --> 46\n",
      "[56, 53, 58, 46] --> 43\n",
      "[56, 53, 58, 46, 43] --> 56\n",
      "[56, 53, 58, 46, 43, 56] --> 10\n",
      "[56, 53, 58, 46, 43, 56, 10] --> 1\n",
      "[56, 53, 58, 46, 43, 56, 10, 1] --> 57\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"{context.tolist()} --> {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "270dc83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2708857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5050, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BiagramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            print(logits.shape)\n",
    "            logits = logits[:,-1,:]\n",
    "            print(logits.shape)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx,idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "model = BiagramLanguageModel(vocab_size)\n",
    "logits,loss = model(xb,yb)\n",
    "print(logits.shape) # (B,T,C)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0be2be89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18533634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 2, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 3, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 4, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 5, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 6, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 7, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 8, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 9, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 10, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 11, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 12, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 13, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 14, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 15, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 16, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 17, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 18, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 19, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 20, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 21, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 22, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 23, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 24, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 25, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 26, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 27, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 28, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 29, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 30, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 31, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 32, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 33, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 34, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 35, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 36, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 37, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 38, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 39, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 40, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 41, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 42, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 43, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 44, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 45, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 46, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 47, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 48, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 49, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 50, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 51, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 52, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 53, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 54, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 55, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 56, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 57, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 58, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 59, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 60, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 61, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 62, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 63, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 64, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 65, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 66, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 67, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 68, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 69, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 70, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 71, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 72, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 73, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 74, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 75, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 76, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 77, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 78, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 79, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 80, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 81, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 82, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 83, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 84, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 85, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 86, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 87, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 88, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 89, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 90, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 91, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 92, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 93, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 94, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 95, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 96, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 97, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 98, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 99, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 100, 65])\n",
      "torch.Size([1, 65])\n",
      "\n",
      "cfYCDRUZsYBsA?Y?vgB!ZWOEiAoezL:q&Avufr?gSGdWrp&Bxt-R?wo'TYhBChdIC-RDaRmEGENyouVg'UjyQNyQSpZUVeN:BZqh\n"
     ]
    }
   ],
   "source": [
    "def start_gen(model:nn.Module, max_new_tokens:int):\n",
    "    print(\"\".join(decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=max_new_tokens)[0].tolist())))\n",
    "start_gen(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "259ce753",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.71400785446167\n",
      "4.697804927825928\n",
      "4.649252414703369\n",
      "4.625182628631592\n",
      "4.4966816902160645\n",
      "4.506692886352539\n",
      "4.634533882141113\n",
      "4.4722466468811035\n",
      "4.682983875274658\n",
      "4.602806091308594\n",
      "4.577833652496338\n",
      "4.643277645111084\n",
      "4.533563613891602\n",
      "4.609025001525879\n",
      "4.483421325683594\n",
      "4.4393415451049805\n",
      "4.562252044677734\n",
      "4.48280143737793\n",
      "4.4507060050964355\n",
      "4.468911647796631\n",
      "4.481171131134033\n",
      "4.451408863067627\n",
      "4.413559913635254\n",
      "4.4951605796813965\n",
      "4.418274402618408\n",
      "4.438932418823242\n",
      "4.375925064086914\n",
      "4.317191123962402\n",
      "4.489402770996094\n",
      "4.480153560638428\n",
      "4.345510005950928\n",
      "4.357389450073242\n",
      "4.279538631439209\n",
      "4.3284807205200195\n",
      "4.356042861938477\n",
      "4.296504020690918\n",
      "4.217926979064941\n",
      "4.342703819274902\n",
      "4.20113468170166\n",
      "4.2905778884887695\n",
      "4.339128017425537\n",
      "4.14097785949707\n",
      "4.224417209625244\n",
      "4.184699058532715\n",
      "4.226337432861328\n",
      "4.222503662109375\n",
      "4.283510208129883\n",
      "4.194325923919678\n",
      "4.140383243560791\n",
      "4.226568222045898\n",
      "4.159667491912842\n",
      "4.101105213165283\n",
      "4.099018096923828\n",
      "4.1389055252075195\n",
      "4.156398773193359\n",
      "4.133793830871582\n",
      "4.130281925201416\n",
      "4.149217128753662\n",
      "4.065577983856201\n",
      "4.0435404777526855\n",
      "4.029851913452148\n",
      "4.140590667724609\n",
      "3.9670114517211914\n",
      "3.953540325164795\n",
      "3.87846302986145\n",
      "3.959343194961548\n",
      "3.9750771522521973\n",
      "3.9198403358459473\n",
      "4.005298137664795\n",
      "4.007288455963135\n",
      "3.983330011367798\n",
      "3.9843392372131348\n",
      "4.044438362121582\n",
      "3.8695173263549805\n",
      "3.87784743309021\n",
      "3.9119133949279785\n",
      "3.963284730911255\n",
      "3.9058010578155518\n",
      "3.8553500175476074\n",
      "3.8852717876434326\n",
      "3.9164228439331055\n",
      "3.802783727645874\n",
      "3.8585591316223145\n",
      "3.761613368988037\n",
      "3.7593679428100586\n",
      "3.7419075965881348\n",
      "3.8515594005584717\n",
      "3.865861177444458\n",
      "3.8376901149749756\n",
      "3.852956533432007\n",
      "3.7900285720825195\n",
      "3.756150245666504\n",
      "3.7560107707977295\n",
      "3.742912769317627\n",
      "3.7213282585144043\n",
      "3.833829879760742\n",
      "3.7866828441619873\n",
      "3.689318895339966\n",
      "3.670889139175415\n",
      "3.791210174560547\n",
      "3.7137200832366943\n",
      "3.698348045349121\n",
      "3.704124689102173\n",
      "3.5919594764709473\n",
      "3.7204041481018066\n",
      "3.7185585498809814\n",
      "3.6811811923980713\n",
      "3.666238307952881\n",
      "3.6513750553131104\n",
      "3.6671810150146484\n",
      "3.629162073135376\n",
      "3.5362143516540527\n",
      "3.6431305408477783\n",
      "3.6053032875061035\n",
      "3.625317335128784\n",
      "3.5374414920806885\n",
      "3.5476813316345215\n",
      "3.5801661014556885\n",
      "3.5726473331451416\n",
      "3.479475498199463\n",
      "3.643278121948242\n",
      "3.652578115463257\n",
      "3.5657989978790283\n",
      "3.551292657852173\n",
      "3.4915823936462402\n",
      "3.4825916290283203\n",
      "3.5896878242492676\n",
      "3.5069143772125244\n",
      "3.4672586917877197\n",
      "3.4556686878204346\n",
      "3.499178171157837\n",
      "3.513190746307373\n",
      "3.3957741260528564\n",
      "3.399317979812622\n",
      "3.5183632373809814\n",
      "3.400707244873047\n",
      "3.452202320098877\n",
      "3.482362747192383\n",
      "3.4330451488494873\n",
      "3.396529197692871\n",
      "3.546340227127075\n",
      "3.4622247219085693\n",
      "3.444856882095337\n",
      "3.4579920768737793\n",
      "3.4152565002441406\n",
      "3.396925926208496\n",
      "3.452085256576538\n",
      "3.4975199699401855\n",
      "3.345094919204712\n",
      "3.3552823066711426\n",
      "3.4421660900115967\n",
      "3.244913339614868\n",
      "3.3817434310913086\n",
      "3.3281195163726807\n",
      "3.310568332672119\n",
      "3.396524429321289\n",
      "3.367567300796509\n",
      "3.211994171142578\n",
      "3.34567928314209\n",
      "3.279104471206665\n",
      "3.2925567626953125\n",
      "3.2418181896209717\n",
      "3.29551362991333\n",
      "3.2944178581237793\n",
      "3.350444793701172\n",
      "3.2792739868164062\n",
      "3.2968637943267822\n",
      "3.2726151943206787\n",
      "3.316455364227295\n",
      "3.2016677856445312\n",
      "3.27706241607666\n",
      "3.378272533416748\n",
      "3.262573003768921\n",
      "3.2394678592681885\n",
      "3.2450313568115234\n",
      "3.2974960803985596\n",
      "3.2830400466918945\n",
      "3.1772735118865967\n",
      "3.241833448410034\n",
      "3.2893686294555664\n",
      "3.1828999519348145\n",
      "3.2688071727752686\n",
      "3.197648048400879\n",
      "3.0901503562927246\n",
      "3.262159824371338\n",
      "3.1461071968078613\n",
      "3.1225178241729736\n",
      "3.1461684703826904\n",
      "3.189744472503662\n",
      "3.1844980716705322\n",
      "3.1086065769195557\n",
      "3.209261655807495\n",
      "3.0719401836395264\n",
      "3.1990182399749756\n",
      "3.1989965438842773\n",
      "3.1035053730010986\n",
      "3.1997005939483643\n",
      "3.1462202072143555\n",
      "3.082202672958374\n",
      "3.1290767192840576\n",
      "3.057305335998535\n",
      "3.1031322479248047\n",
      "3.0832741260528564\n",
      "3.154658555984497\n",
      "3.1391258239746094\n",
      "3.1235134601593018\n",
      "3.0486347675323486\n",
      "3.122770309448242\n",
      "3.180569887161255\n",
      "3.019587278366089\n",
      "2.9796712398529053\n",
      "3.0839712619781494\n",
      "3.1122543811798096\n",
      "3.01478910446167\n",
      "3.0109283924102783\n",
      "3.1373910903930664\n",
      "3.048139810562134\n",
      "3.0097312927246094\n",
      "3.0358030796051025\n",
      "2.9560670852661133\n",
      "3.1512515544891357\n",
      "3.084697961807251\n",
      "2.9431800842285156\n",
      "3.0304462909698486\n",
      "2.9805240631103516\n",
      "3.0845372676849365\n",
      "3.1503376960754395\n",
      "3.0748708248138428\n",
      "3.041266918182373\n",
      "3.029681444168091\n",
      "2.999840021133423\n",
      "3.003647804260254\n",
      "3.0455322265625\n",
      "2.9976072311401367\n",
      "2.9970521926879883\n",
      "3.04164457321167\n",
      "2.9072375297546387\n",
      "2.9815192222595215\n",
      "3.0321414470672607\n",
      "2.9725565910339355\n",
      "2.9924967288970947\n",
      "2.9337854385375977\n",
      "2.9376473426818848\n",
      "2.8789737224578857\n",
      "2.9556193351745605\n",
      "3.0381128787994385\n",
      "3.0066356658935547\n",
      "2.895902633666992\n",
      "3.077280282974243\n",
      "2.8246655464172363\n",
      "2.9097869396209717\n",
      "2.9638984203338623\n",
      "2.9001636505126953\n",
      "2.997339963912964\n",
      "2.9255869388580322\n",
      "2.9582653045654297\n",
      "2.8940491676330566\n",
      "2.7990381717681885\n",
      "2.8657541275024414\n",
      "2.9507670402526855\n",
      "2.983588695526123\n",
      "2.8887927532196045\n",
      "2.919135332107544\n",
      "2.781237840652466\n",
      "2.86647891998291\n",
      "2.9005093574523926\n",
      "2.9669713973999023\n",
      "2.9070770740509033\n",
      "2.8446311950683594\n",
      "2.8923356533050537\n",
      "2.9437761306762695\n",
      "2.9643394947052\n",
      "2.860109329223633\n",
      "2.808502197265625\n",
      "2.877855062484741\n",
      "2.9365580081939697\n",
      "2.9913246631622314\n",
      "2.9125654697418213\n",
      "2.965411424636841\n",
      "2.890230417251587\n",
      "3.010538339614868\n",
      "2.839138984680176\n",
      "2.8440284729003906\n",
      "2.9773519039154053\n",
      "2.7623062133789062\n",
      "2.86997127532959\n",
      "2.906139850616455\n",
      "2.9267234802246094\n",
      "2.8005762100219727\n",
      "2.7972004413604736\n",
      "2.8907365798950195\n",
      "2.7333264350891113\n",
      "2.8963398933410645\n",
      "2.7514548301696777\n",
      "2.8929073810577393\n",
      "2.874890089035034\n",
      "2.6993720531463623\n",
      "2.778576612472534\n",
      "2.84739351272583\n",
      "2.816073417663574\n",
      "2.9214539527893066\n",
      "2.7978854179382324\n",
      "2.8235907554626465\n",
      "2.891477346420288\n",
      "2.802305221557617\n",
      "2.755309820175171\n",
      "2.76947021484375\n",
      "2.7991414070129395\n",
      "2.8173468112945557\n",
      "2.7318389415740967\n",
      "2.7844455242156982\n",
      "2.837458610534668\n",
      "2.8707306385040283\n",
      "2.756762742996216\n",
      "2.7086598873138428\n",
      "2.7080984115600586\n",
      "2.843393564224243\n",
      "2.820305109024048\n",
      "2.7883293628692627\n",
      "2.8727993965148926\n",
      "2.7041404247283936\n",
      "2.849515199661255\n",
      "2.74989914894104\n",
      "2.8016254901885986\n",
      "2.801643133163452\n",
      "2.77842378616333\n",
      "2.818768262863159\n",
      "2.777371406555176\n",
      "2.703428030014038\n",
      "2.743436098098755\n",
      "2.816098928451538\n",
      "2.728346109390259\n",
      "2.791567325592041\n",
      "2.765753746032715\n",
      "2.7721493244171143\n",
      "2.7245261669158936\n",
      "2.6647098064422607\n",
      "2.899195671081543\n",
      "2.7214040756225586\n",
      "2.845524549484253\n",
      "2.6435821056365967\n",
      "2.7230377197265625\n",
      "2.7025370597839355\n",
      "2.665895700454712\n",
      "2.7918176651000977\n",
      "2.9094882011413574\n",
      "2.7201004028320312\n",
      "2.649975538253784\n",
      "2.8082520961761475\n",
      "2.7352216243743896\n",
      "2.7333786487579346\n",
      "2.6360409259796143\n",
      "2.7612955570220947\n",
      "2.827942371368408\n",
      "2.74056339263916\n",
      "2.8789241313934326\n",
      "2.6580655574798584\n",
      "2.7269697189331055\n",
      "2.6965718269348145\n",
      "2.7782492637634277\n",
      "2.7412796020507812\n",
      "2.7866370677948\n",
      "2.724886894226074\n",
      "2.7016379833221436\n",
      "2.6273434162139893\n",
      "2.819207191467285\n",
      "2.7317757606506348\n",
      "2.79430890083313\n",
      "2.7510006427764893\n",
      "2.6329896450042725\n",
      "2.628509998321533\n",
      "2.7434558868408203\n",
      "2.6705093383789062\n",
      "2.788653612136841\n",
      "2.6011462211608887\n",
      "2.653397798538208\n",
      "2.7041513919830322\n",
      "2.6681113243103027\n",
      "2.5521135330200195\n",
      "2.7219672203063965\n",
      "2.709371566772461\n",
      "2.602703094482422\n",
      "2.695260763168335\n",
      "2.678985834121704\n",
      "2.5898845195770264\n",
      "2.7462503910064697\n",
      "2.676706552505493\n",
      "2.708353281021118\n",
      "2.730619192123413\n",
      "2.6686277389526367\n",
      "2.661146879196167\n",
      "2.5758371353149414\n",
      "2.6475930213928223\n",
      "2.574387788772583\n",
      "2.7127671241760254\n",
      "2.681941032409668\n",
      "2.7024598121643066\n",
      "2.553990125656128\n",
      "2.637768507003784\n",
      "2.6652414798736572\n",
      "2.736888885498047\n",
      "2.7152795791625977\n",
      "2.595371723175049\n",
      "2.7208571434020996\n",
      "2.577810287475586\n",
      "2.6513147354125977\n",
      "2.668402671813965\n",
      "2.749001979827881\n",
      "2.634962320327759\n",
      "2.5563712120056152\n",
      "2.713843822479248\n",
      "2.808131217956543\n",
      "2.80777907371521\n",
      "2.6334712505340576\n",
      "2.654975175857544\n",
      "2.5564370155334473\n",
      "2.6482763290405273\n",
      "2.7317845821380615\n",
      "2.5373125076293945\n",
      "2.642033815383911\n",
      "2.7095634937286377\n",
      "2.5282223224639893\n",
      "2.7519824504852295\n",
      "2.648613691329956\n",
      "2.667090654373169\n",
      "2.633209705352783\n",
      "2.5928828716278076\n",
      "2.65590763092041\n",
      "2.4437367916107178\n",
      "2.628143548965454\n",
      "2.637969732284546\n",
      "2.616568088531494\n",
      "2.8489787578582764\n",
      "2.60663104057312\n",
      "2.6714398860931396\n",
      "2.69966721534729\n",
      "2.5842442512512207\n",
      "2.548236846923828\n",
      "2.606088399887085\n",
      "2.6423966884613037\n",
      "2.607100486755371\n",
      "2.634464740753174\n",
      "2.697117328643799\n",
      "2.5456795692443848\n",
      "2.6484475135803223\n",
      "2.551232099533081\n",
      "2.5642967224121094\n",
      "2.6164326667785645\n",
      "2.6274492740631104\n",
      "2.6045541763305664\n",
      "2.681708574295044\n",
      "2.7820754051208496\n",
      "2.646604061126709\n",
      "2.6745684146881104\n",
      "2.564976453781128\n",
      "2.606106996536255\n",
      "2.6718878746032715\n",
      "2.4689078330993652\n",
      "2.5953798294067383\n",
      "2.5959084033966064\n",
      "2.576380729675293\n",
      "2.577219009399414\n",
      "2.6458194255828857\n",
      "2.525498151779175\n",
      "2.6911914348602295\n",
      "2.7205326557159424\n",
      "2.6916842460632324\n",
      "2.5413150787353516\n",
      "2.611341953277588\n",
      "2.6165921688079834\n",
      "2.6029748916625977\n",
      "2.5085716247558594\n",
      "2.75811505317688\n",
      "2.4940004348754883\n",
      "2.7217817306518555\n",
      "2.5063669681549072\n",
      "2.594132423400879\n",
      "2.6335175037384033\n",
      "2.5043985843658447\n",
      "2.489213228225708\n",
      "2.7017674446105957\n",
      "2.5679569244384766\n",
      "2.725409507751465\n",
      "2.533893346786499\n",
      "2.719376564025879\n",
      "2.601956844329834\n",
      "2.5496413707733154\n",
      "2.586880683898926\n",
      "2.4523141384124756\n",
      "2.5560238361358643\n",
      "2.6605539321899414\n",
      "2.7299892902374268\n",
      "2.6614255905151367\n",
      "2.5354766845703125\n",
      "2.5162265300750732\n",
      "2.7465860843658447\n",
      "2.512425422668457\n",
      "2.4927544593811035\n",
      "2.6685173511505127\n",
      "2.5308163166046143\n",
      "2.6789450645446777\n",
      "2.523732900619507\n",
      "2.6551413536071777\n",
      "2.471644401550293\n",
      "2.518775463104248\n",
      "2.5223922729492188\n",
      "2.5431466102600098\n",
      "2.532986879348755\n",
      "2.515409231185913\n",
      "2.5522537231445312\n",
      "2.5626423358917236\n",
      "2.653459072113037\n",
      "2.5730113983154297\n",
      "2.5275044441223145\n",
      "2.601154088973999\n",
      "2.5556282997131348\n",
      "2.5948569774627686\n",
      "2.5082569122314453\n",
      "2.5510175228118896\n",
      "2.5332133769989014\n",
      "2.54555344581604\n",
      "2.583516836166382\n",
      "2.4360191822052\n",
      "2.559175729751587\n",
      "2.512377977371216\n",
      "2.614384889602661\n",
      "2.601367473602295\n",
      "2.6396098136901855\n",
      "2.615217447280884\n",
      "2.543667793273926\n",
      "2.5973317623138428\n",
      "2.5063531398773193\n",
      "2.569049835205078\n",
      "2.545180559158325\n",
      "2.6270546913146973\n",
      "2.4955265522003174\n",
      "2.4955685138702393\n",
      "2.675523042678833\n",
      "2.479796886444092\n",
      "2.5319223403930664\n",
      "2.530343532562256\n",
      "2.5746798515319824\n",
      "2.4429752826690674\n",
      "2.540452480316162\n",
      "2.583170175552368\n",
      "2.423983097076416\n",
      "2.5233266353607178\n",
      "2.5507314205169678\n",
      "2.5230777263641357\n",
      "2.6207022666931152\n",
      "2.582411050796509\n",
      "2.7321746349334717\n",
      "2.5010018348693848\n",
      "2.52482533454895\n",
      "2.6039741039276123\n",
      "2.566883087158203\n",
      "2.545534133911133\n",
      "2.507181406021118\n",
      "2.623608350753784\n",
      "2.6255786418914795\n",
      "2.5705318450927734\n",
      "2.627824306488037\n",
      "2.4405717849731445\n",
      "2.552060604095459\n",
      "2.595953941345215\n",
      "2.3826959133148193\n",
      "2.6296579837799072\n",
      "2.5603599548339844\n",
      "2.5029783248901367\n",
      "2.5491888523101807\n",
      "2.5093586444854736\n",
      "2.6127283573150635\n",
      "2.5468995571136475\n",
      "2.5586297512054443\n",
      "2.5718674659729004\n",
      "2.5377519130706787\n",
      "2.6146531105041504\n",
      "2.5351197719573975\n",
      "2.6005990505218506\n",
      "2.619405746459961\n",
      "2.6131515502929688\n",
      "2.4936678409576416\n",
      "2.699939489364624\n",
      "2.5097501277923584\n",
      "2.6359121799468994\n",
      "2.601985216140747\n",
      "2.5288331508636475\n",
      "2.542670249938965\n",
      "2.4929392337799072\n",
      "2.4687864780426025\n",
      "2.528792142868042\n",
      "2.5811073780059814\n",
      "2.5178561210632324\n",
      "2.6156017780303955\n",
      "2.485203266143799\n",
      "2.590857744216919\n",
      "2.481806993484497\n",
      "2.5758183002471924\n",
      "2.5933568477630615\n",
      "2.5369277000427246\n",
      "2.5226457118988037\n",
      "2.5405449867248535\n",
      "2.4186675548553467\n",
      "2.4844088554382324\n",
      "2.5495200157165527\n",
      "2.467193603515625\n",
      "2.4941766262054443\n",
      "2.629880905151367\n",
      "2.596010446548462\n",
      "2.658013105392456\n",
      "2.451503038406372\n",
      "2.593470811843872\n",
      "2.6064069271087646\n",
      "2.5550827980041504\n",
      "2.6103675365448\n",
      "2.647085666656494\n",
      "2.623479127883911\n",
      "2.5649070739746094\n",
      "2.6377882957458496\n",
      "2.5408408641815186\n",
      "2.5515267848968506\n",
      "2.4661285877227783\n",
      "2.623777389526367\n",
      "2.5192337036132812\n",
      "2.471966505050659\n",
      "2.64144229888916\n",
      "2.5573925971984863\n",
      "2.5866880416870117\n",
      "2.611584424972534\n",
      "2.615126371383667\n",
      "2.6225533485412598\n",
      "2.486382484436035\n",
      "2.570084571838379\n",
      "2.513713836669922\n",
      "2.486149787902832\n",
      "2.460702419281006\n",
      "2.5860848426818848\n",
      "2.512991428375244\n",
      "2.674687623977661\n",
      "2.4723997116088867\n",
      "2.495283603668213\n",
      "2.4494733810424805\n",
      "2.530475616455078\n",
      "2.5615148544311523\n",
      "2.717130184173584\n",
      "2.491431951522827\n",
      "2.4962637424468994\n",
      "2.579257011413574\n",
      "2.5208580493927\n",
      "2.4386894702911377\n",
      "2.67167067527771\n",
      "2.6350760459899902\n",
      "2.633424758911133\n",
      "2.5350494384765625\n",
      "2.6187543869018555\n",
      "2.4010775089263916\n",
      "2.5532569885253906\n",
      "2.573350191116333\n",
      "2.5000720024108887\n",
      "2.4210381507873535\n",
      "2.489567756652832\n",
      "2.594350576400757\n",
      "2.53950834274292\n",
      "2.5671112537384033\n",
      "2.5964930057525635\n",
      "2.5963239669799805\n",
      "2.447699546813965\n",
      "2.519150495529175\n",
      "2.538290023803711\n",
      "2.633155584335327\n",
      "2.604808807373047\n",
      "2.4733681678771973\n",
      "2.5914738178253174\n",
      "2.5350632667541504\n",
      "2.62528395652771\n",
      "2.568037986755371\n",
      "2.5251851081848145\n",
      "2.6791374683380127\n",
      "2.4238321781158447\n",
      "2.402026653289795\n",
      "2.4988696575164795\n",
      "2.6744096279144287\n",
      "2.539924144744873\n",
      "2.5579476356506348\n",
      "2.484604835510254\n",
      "2.552478551864624\n",
      "2.4601218700408936\n",
      "2.611419677734375\n",
      "2.484208583831787\n",
      "2.540950059890747\n",
      "2.521911382675171\n",
      "2.5687241554260254\n",
      "2.6967735290527344\n",
      "2.5882527828216553\n",
      "2.4024643898010254\n",
      "2.6767427921295166\n",
      "2.470252275466919\n",
      "2.550349235534668\n",
      "2.462906837463379\n",
      "2.540242910385132\n",
      "2.575723886489868\n",
      "2.4658203125\n",
      "2.4775214195251465\n",
      "2.5785040855407715\n",
      "2.569685935974121\n",
      "2.562319755554199\n",
      "2.566284418106079\n",
      "2.546006441116333\n",
      "2.6728832721710205\n",
      "2.454913377761841\n",
      "2.55725359916687\n",
      "2.6209826469421387\n",
      "2.62300443649292\n",
      "2.565824508666992\n",
      "2.405045986175537\n",
      "2.600215435028076\n",
      "2.431591510772705\n",
      "2.4925625324249268\n",
      "2.6236608028411865\n",
      "2.455120086669922\n",
      "2.584602117538452\n",
      "2.471741199493408\n",
      "2.5643019676208496\n",
      "2.4563333988189697\n",
      "2.4518470764160156\n",
      "2.4516468048095703\n",
      "2.625816583633423\n",
      "2.543414831161499\n",
      "2.4905173778533936\n",
      "2.4173784255981445\n",
      "2.4603798389434814\n",
      "2.4808943271636963\n",
      "2.5013811588287354\n",
      "2.5333216190338135\n",
      "2.476330518722534\n",
      "2.5189502239227295\n",
      "2.5050482749938965\n",
      "2.5174176692962646\n",
      "2.4822468757629395\n",
      "2.5264461040496826\n",
      "2.6525182723999023\n",
      "2.557107925415039\n",
      "2.4016079902648926\n",
      "2.542635440826416\n",
      "2.5857789516448975\n",
      "2.589623212814331\n",
      "2.3849544525146484\n",
      "2.4927783012390137\n",
      "2.4903390407562256\n",
      "2.619657516479492\n",
      "2.397697687149048\n",
      "2.5554933547973633\n",
      "2.3912863731384277\n",
      "2.4367878437042236\n",
      "2.4775760173797607\n",
      "2.486936092376709\n",
      "2.4632458686828613\n",
      "2.528212308883667\n",
      "2.593496799468994\n",
      "2.524923801422119\n",
      "2.6498992443084717\n",
      "2.5430591106414795\n",
      "2.328592300415039\n",
      "2.518181324005127\n",
      "2.605044364929199\n",
      "2.500434398651123\n",
      "2.6019225120544434\n",
      "2.4473717212677\n",
      "2.4655063152313232\n",
      "2.502265214920044\n",
      "2.5232198238372803\n",
      "2.570862293243408\n",
      "2.4577205181121826\n",
      "2.5640969276428223\n",
      "2.3830344676971436\n",
      "2.430605411529541\n",
      "2.551267147064209\n",
      "2.4948151111602783\n",
      "2.476698875427246\n",
      "2.5860798358917236\n",
      "2.448523998260498\n",
      "2.531196117401123\n",
      "2.5525400638580322\n",
      "2.5069210529327393\n",
      "2.525960922241211\n",
      "2.5261471271514893\n",
      "2.5278713703155518\n",
      "2.5825202465057373\n",
      "2.449659824371338\n",
      "2.4711878299713135\n",
      "2.5398142337799072\n",
      "2.445902109146118\n",
      "2.4354920387268066\n",
      "2.5242645740509033\n",
      "2.543936252593994\n",
      "2.477931261062622\n",
      "2.518458604812622\n",
      "2.5000288486480713\n",
      "2.640947103500366\n",
      "2.4980294704437256\n",
      "2.636077404022217\n",
      "2.512730121612549\n",
      "2.5307931900024414\n",
      "2.4689977169036865\n",
      "2.4498846530914307\n",
      "2.494596004486084\n",
      "2.4817309379577637\n",
      "2.493809223175049\n",
      "2.5293350219726562\n",
      "2.6037654876708984\n",
      "2.47007155418396\n",
      "2.536846160888672\n",
      "2.5250654220581055\n",
      "2.4200663566589355\n",
      "2.448031425476074\n",
      "2.5601606369018555\n",
      "2.3797426223754883\n",
      "2.501535177230835\n",
      "2.424955368041992\n",
      "2.4259016513824463\n",
      "2.4257097244262695\n",
      "2.6402971744537354\n",
      "2.535274028778076\n",
      "2.5248992443084717\n",
      "2.3853042125701904\n",
      "2.5786609649658203\n",
      "2.577402353286743\n",
      "2.5631771087646484\n",
      "2.4128448963165283\n",
      "2.5182626247406006\n",
      "2.5623984336853027\n",
      "2.6195290088653564\n",
      "2.4335949420928955\n",
      "2.548621654510498\n",
      "2.4593231678009033\n",
      "2.4687554836273193\n",
      "2.3452861309051514\n",
      "2.463599443435669\n",
      "2.541743755340576\n",
      "2.4230213165283203\n",
      "2.5539677143096924\n",
      "2.5435116291046143\n",
      "2.6282362937927246\n",
      "2.3463027477264404\n",
      "2.6153833866119385\n",
      "2.433497428894043\n",
      "2.4044253826141357\n",
      "2.488177537918091\n",
      "2.464259624481201\n",
      "2.4973549842834473\n",
      "2.499690294265747\n",
      "2.5313451290130615\n",
      "2.5060324668884277\n",
      "2.6130902767181396\n",
      "2.4901537895202637\n",
      "2.4492852687835693\n",
      "2.5108234882354736\n",
      "2.6985833644866943\n",
      "2.460493326187134\n",
      "2.6371772289276123\n",
      "2.389780282974243\n",
      "2.420879602432251\n",
      "2.380282402038574\n",
      "2.4627349376678467\n",
      "2.5532729625701904\n",
      "2.564631223678589\n",
      "2.4954497814178467\n",
      "2.470964193344116\n",
      "2.5017902851104736\n",
      "2.505141258239746\n",
      "2.3537943363189697\n",
      "2.4229071140289307\n",
      "2.5767014026641846\n",
      "2.4610252380371094\n",
      "2.610156774520874\n",
      "2.3787357807159424\n",
      "2.533168315887451\n",
      "2.483625888824463\n",
      "2.587007522583008\n",
      "2.406883478164673\n",
      "2.3413777351379395\n",
      "2.4793179035186768\n",
      "2.3987574577331543\n",
      "2.491433620452881\n",
      "2.517524003982544\n",
      "2.4494645595550537\n",
      "2.475797176361084\n",
      "2.6170904636383057\n",
      "2.4643447399139404\n",
      "2.511580228805542\n",
      "2.4950177669525146\n",
      "2.3747010231018066\n",
      "2.491888999938965\n",
      "2.526332378387451\n",
      "2.406423568725586\n",
      "2.442239761352539\n",
      "2.535399913787842\n",
      "2.4918038845062256\n",
      "2.5175485610961914\n",
      "2.3749637603759766\n",
      "2.482219696044922\n",
      "2.518238067626953\n",
      "2.406921148300171\n",
      "2.553555488586426\n",
      "2.492830991744995\n",
      "2.5280721187591553\n",
      "2.483548164367676\n",
      "2.527862787246704\n",
      "2.6300110816955566\n",
      "2.427156448364258\n",
      "2.5338518619537354\n",
      "2.556636333465576\n",
      "2.5306875705718994\n",
      "2.5747177600860596\n",
      "2.402398109436035\n",
      "2.5631914138793945\n",
      "2.4762630462646484\n",
      "2.559020519256592\n",
      "2.638395071029663\n",
      "2.4563066959381104\n",
      "2.507168769836426\n",
      "2.5097224712371826\n",
      "2.5174753665924072\n",
      "2.5408105850219727\n",
      "2.5667688846588135\n",
      "2.308601140975952\n",
      "2.626471519470215\n",
      "2.545563220977783\n",
      "2.5296366214752197\n",
      "2.4127423763275146\n",
      "2.4876315593719482\n",
      "2.451925754547119\n",
      "2.459235191345215\n",
      "2.5483596324920654\n",
      "2.5580081939697266\n",
      "2.4047768115997314\n",
      "2.4370944499969482\n",
      "2.4517252445220947\n",
      "2.5108642578125\n",
      "2.581752300262451\n",
      "2.495514392852783\n",
      "2.4855458736419678\n",
      "2.5303263664245605\n",
      "2.501866579055786\n",
      "2.4702417850494385\n",
      "2.5728518962860107\n",
      "2.5618724822998047\n",
      "2.4536187648773193\n",
      "2.4618849754333496\n",
      "2.5497019290924072\n",
      "2.593341827392578\n",
      "2.3627328872680664\n",
      "2.546572685241699\n",
      "2.559424877166748\n",
      "2.3666632175445557\n",
      "2.4735829830169678\n",
      "2.33262300491333\n",
      "2.406999111175537\n",
      "2.450690984725952\n",
      "2.540811061859131\n",
      "2.4011940956115723\n",
      "2.509183645248413\n",
      "2.549283027648926\n",
      "2.5741889476776123\n",
      "2.4710915088653564\n",
      "2.4021377563476562\n",
      "2.408151626586914\n",
      "2.565736770629883\n",
      "2.5245444774627686\n",
      "2.4836044311523438\n",
      "2.580612897872925\n",
      "2.558400869369507\n",
      "2.4895496368408203\n",
      "2.4201087951660156\n",
      "2.541213274002075\n",
      "2.4627785682678223\n",
      "2.5381436347961426\n",
      "2.5436959266662598\n",
      "2.3912596702575684\n",
      "2.4348721504211426\n",
      "2.49375319480896\n",
      "2.576958656311035\n",
      "2.368868589401245\n",
      "2.55244517326355\n",
      "2.496539354324341\n",
      "2.400805711746216\n",
      "2.56801438331604\n",
      "2.458827495574951\n",
      "2.5074634552001953\n",
      "2.48642635345459\n",
      "2.655318021774292\n",
      "2.471827745437622\n",
      "2.6877710819244385\n",
      "2.533076286315918\n",
      "2.462594985961914\n",
      "2.500331401824951\n",
      "2.6545932292938232\n",
      "2.5380077362060547\n",
      "2.5665643215179443\n",
      "2.4530391693115234\n",
      "2.4945526123046875\n",
      "2.4056668281555176\n",
      "2.5579681396484375\n",
      "2.5025579929351807\n",
      "2.4700329303741455\n",
      "2.609788417816162\n",
      "2.4798038005828857\n",
      "2.5720503330230713\n",
      "2.5968074798583984\n",
      "2.60929536819458\n",
      "2.4553356170654297\n",
      "2.482228994369507\n",
      "2.4764797687530518\n",
      "2.450441598892212\n",
      "2.4610700607299805\n",
      "2.5993828773498535\n",
      "2.5614073276519775\n",
      "2.4771010875701904\n",
      "2.548231601715088\n",
      "2.4604380130767822\n",
      "2.4222543239593506\n",
      "2.4666919708251953\n",
      "2.4493138790130615\n",
      "2.552868127822876\n",
      "2.568047285079956\n",
      "2.484499216079712\n",
      "2.4928338527679443\n",
      "2.5749664306640625\n",
      "2.496887683868408\n",
      "2.4622442722320557\n",
      "2.614302635192871\n",
      "2.4159226417541504\n",
      "2.597792863845825\n",
      "2.609649419784546\n",
      "2.4344847202301025\n",
      "2.4870026111602783\n",
      "2.6382510662078857\n",
      "2.4626543521881104\n",
      "2.566682815551758\n",
      "2.553762912750244\n",
      "2.361734390258789\n",
      "2.5102641582489014\n",
      "2.5654611587524414\n",
      "2.406456708908081\n",
      "2.6644983291625977\n",
      "2.527247905731201\n",
      "2.6108782291412354\n",
      "2.4071249961853027\n",
      "2.5144622325897217\n",
      "2.5626065731048584\n",
      "2.6245791912078857\n",
      "2.4985973834991455\n",
      "2.471735715866089\n",
      "2.505605697631836\n",
      "2.4158456325531006\n",
      "2.51625657081604\n",
      "2.5642478466033936\n",
      "2.5649573802948\n",
      "2.519138813018799\n",
      "2.5918588638305664\n",
      "2.5744810104370117\n",
      "2.535576343536377\n",
      "2.6408884525299072\n",
      "2.532745599746704\n",
      "2.5005996227264404\n",
      "2.5481626987457275\n",
      "2.5256354808807373\n",
      "2.5161235332489014\n",
      "2.4745469093322754\n",
      "2.46657395362854\n",
      "2.510817050933838\n",
      "2.462440252304077\n",
      "2.4935550689697266\n",
      "2.543484926223755\n",
      "2.440290689468384\n",
      "2.507540225982666\n",
      "2.4997143745422363\n",
      "2.4953742027282715\n",
      "2.4240875244140625\n",
      "2.6287295818328857\n",
      "2.6211254596710205\n",
      "2.5015900135040283\n",
      "2.301257371902466\n",
      "2.392746925354004\n",
      "2.4465882778167725\n",
      "2.6016430854797363\n",
      "2.4361517429351807\n",
      "2.426941156387329\n",
      "2.5076632499694824\n",
      "2.519989252090454\n",
      "2.5789990425109863\n",
      "2.4331204891204834\n",
      "2.577869176864624\n",
      "2.460136651992798\n",
      "2.6094460487365723\n",
      "2.6615374088287354\n",
      "2.460498571395874\n",
      "2.6147708892822266\n",
      "2.4463212490081787\n",
      "2.5932183265686035\n",
      "2.4708449840545654\n",
      "2.4396586418151855\n",
      "2.510894775390625\n",
      "2.55029034614563\n",
      "2.6333820819854736\n",
      "2.555237293243408\n",
      "2.508683204650879\n",
      "2.3927857875823975\n",
      "2.3786513805389404\n",
      "2.4079058170318604\n",
      "2.603736162185669\n",
      "2.3890411853790283\n",
      "2.611952066421509\n",
      "2.574204683303833\n",
      "2.3990864753723145\n",
      "2.442047119140625\n",
      "2.426576614379883\n",
      "2.4415881633758545\n",
      "2.403766632080078\n",
      "2.5896918773651123\n",
      "2.573225975036621\n",
      "2.3995723724365234\n",
      "2.4825117588043213\n",
      "2.5977871417999268\n",
      "2.48130464553833\n",
      "2.4139866828918457\n",
      "2.521317720413208\n",
      "2.474759817123413\n",
      "2.5357601642608643\n",
      "2.434415578842163\n",
      "2.5134940147399902\n",
      "2.4762768745422363\n",
      "2.4717233180999756\n",
      "2.5654044151306152\n",
      "2.4110586643218994\n",
      "2.5632059574127197\n",
      "2.477332592010498\n",
      "2.4681148529052734\n",
      "2.5037736892700195\n",
      "2.4695510864257812\n",
      "2.412654161453247\n",
      "2.581789970397949\n",
      "2.5576279163360596\n",
      "2.480023145675659\n",
      "2.5489840507507324\n",
      "2.4329020977020264\n",
      "2.46150541305542\n",
      "2.5186614990234375\n",
      "2.364386796951294\n",
      "2.513979434967041\n",
      "2.360511064529419\n",
      "2.5861241817474365\n",
      "2.3681204319000244\n",
      "2.535487651824951\n",
      "2.4932985305786133\n",
      "2.4241037368774414\n",
      "2.4194533824920654\n",
      "2.4229371547698975\n",
      "2.489396572113037\n",
      "2.5286762714385986\n",
      "2.3778600692749023\n",
      "2.540154457092285\n",
      "2.595000982284546\n",
      "2.5606610774993896\n",
      "2.4697158336639404\n",
      "2.5027287006378174\n",
      "2.4789514541625977\n",
      "2.358883857727051\n",
      "2.413837194442749\n",
      "2.4662907123565674\n",
      "2.5679149627685547\n",
      "2.547961950302124\n",
      "2.4090566635131836\n",
      "2.418487787246704\n",
      "2.406805992126465\n",
      "2.406792402267456\n",
      "2.403155565261841\n",
      "2.3801331520080566\n",
      "2.5277721881866455\n",
      "2.4905498027801514\n",
      "2.593498468399048\n",
      "2.339304208755493\n",
      "2.614811420440674\n",
      "2.5123157501220703\n",
      "2.436566114425659\n",
      "2.5063087940216064\n",
      "2.3879525661468506\n",
      "2.5725629329681396\n",
      "2.482386589050293\n",
      "2.5803794860839844\n",
      "2.5631158351898193\n",
      "2.4963767528533936\n",
      "2.4222183227539062\n",
      "2.437201738357544\n",
      "2.4369637966156006\n",
      "2.5037081241607666\n",
      "2.452808141708374\n",
      "2.466848134994507\n",
      "2.4695682525634766\n",
      "2.3658039569854736\n",
      "2.5157485008239746\n",
      "2.5075976848602295\n",
      "2.5581443309783936\n",
      "2.453557014465332\n",
      "2.531245231628418\n",
      "2.389139413833618\n",
      "2.4304416179656982\n",
      "2.4701621532440186\n",
      "2.492018938064575\n",
      "2.49961519241333\n",
      "2.3105149269104004\n",
      "2.391530990600586\n",
      "2.5397515296936035\n",
      "2.4463706016540527\n",
      "2.34885835647583\n",
      "2.5443859100341797\n",
      "2.4519052505493164\n",
      "2.593762159347534\n",
      "2.3902461528778076\n",
      "2.4599647521972656\n",
      "2.5673606395721436\n",
      "2.504906415939331\n",
      "2.485718250274658\n",
      "2.516573667526245\n",
      "2.4302773475646973\n",
      "2.5272202491760254\n",
      "2.548801898956299\n",
      "2.4867169857025146\n",
      "2.539309501647949\n",
      "2.537752628326416\n",
      "2.393639326095581\n",
      "2.501887798309326\n",
      "2.5401406288146973\n",
      "2.524620771408081\n",
      "2.422332525253296\n",
      "2.4725425243377686\n",
      "2.4402291774749756\n",
      "2.473893165588379\n",
      "2.595456600189209\n",
      "2.5489087104797363\n",
      "2.4446866512298584\n",
      "2.439307689666748\n",
      "2.406395673751831\n",
      "2.4709620475769043\n",
      "2.6176321506500244\n",
      "2.491642475128174\n",
      "2.5326058864593506\n",
      "2.641374111175537\n",
      "2.4971096515655518\n",
      "2.4463298320770264\n",
      "2.400486707687378\n",
      "2.4223387241363525\n",
      "2.4712047576904297\n",
      "2.533247709274292\n",
      "2.5607805252075195\n",
      "2.440054416656494\n",
      "2.5035362243652344\n",
      "2.416393518447876\n",
      "2.45644474029541\n",
      "2.3484318256378174\n",
      "2.429534673690796\n",
      "2.4736454486846924\n",
      "2.6301982402801514\n",
      "2.514561891555786\n",
      "2.523721933364868\n",
      "2.5215864181518555\n",
      "2.457956314086914\n",
      "2.481618881225586\n",
      "2.4828736782073975\n",
      "2.463660955429077\n",
      "2.5006520748138428\n",
      "2.4033868312835693\n",
      "2.5120091438293457\n",
      "2.50512433052063\n",
      "2.4839248657226562\n",
      "2.4637815952301025\n",
      "2.554352283477783\n",
      "2.4565553665161133\n",
      "2.363642930984497\n",
      "2.5304598808288574\n",
      "2.485529899597168\n",
      "2.5660529136657715\n",
      "2.388378143310547\n",
      "2.3932995796203613\n",
      "2.275691032409668\n",
      "2.4322867393493652\n",
      "2.5145533084869385\n",
      "2.4893527030944824\n",
      "2.5196046829223633\n",
      "2.3658483028411865\n",
      "2.480825424194336\n",
      "2.472255229949951\n",
      "2.5507450103759766\n",
      "2.52156662940979\n",
      "2.5409908294677734\n",
      "2.4826276302337646\n",
      "2.472282648086548\n",
      "2.4396464824676514\n",
      "2.545074701309204\n",
      "2.531564712524414\n",
      "2.547743320465088\n",
      "2.491140604019165\n",
      "2.3089687824249268\n",
      "2.5795891284942627\n",
      "2.38215970993042\n",
      "2.542119264602661\n",
      "2.4214069843292236\n",
      "2.5261292457580566\n",
      "2.5521278381347656\n",
      "2.3959391117095947\n",
      "2.489227533340454\n",
      "2.585409641265869\n",
      "2.5896267890930176\n",
      "2.4331111907958984\n",
      "2.5149669647216797\n",
      "2.412379264831543\n",
      "2.5208182334899902\n",
      "2.422971487045288\n",
      "2.5046279430389404\n",
      "2.5124311447143555\n",
      "2.4541823863983154\n",
      "2.455622911453247\n",
      "2.5841379165649414\n",
      "2.4027581214904785\n",
      "2.368645191192627\n",
      "2.498739242553711\n",
      "2.3502182960510254\n",
      "2.5114848613739014\n",
      "2.4626193046569824\n",
      "2.4785852432250977\n",
      "2.5527586936950684\n",
      "2.3953726291656494\n",
      "2.4601948261260986\n",
      "2.502596855163574\n",
      "2.5366759300231934\n",
      "2.5464279651641846\n",
      "2.4100375175476074\n",
      "2.5512917041778564\n",
      "2.5189473628997803\n",
      "2.4168407917022705\n",
      "2.4352777004241943\n",
      "2.4810402393341064\n",
      "2.4163529872894287\n",
      "2.3392224311828613\n",
      "2.40421724319458\n",
      "2.402491331100464\n",
      "2.5487594604492188\n",
      "2.4393601417541504\n",
      "2.3846356868743896\n",
      "2.637563705444336\n",
      "2.551980495452881\n",
      "2.5397770404815674\n",
      "2.443115472793579\n",
      "2.46829891204834\n",
      "2.519710063934326\n",
      "2.4139082431793213\n",
      "2.511566400527954\n",
      "2.4248416423797607\n",
      "2.4681270122528076\n",
      "2.4279367923736572\n",
      "2.567138433456421\n",
      "2.424210786819458\n",
      "2.438308000564575\n",
      "2.408250570297241\n",
      "2.5025558471679688\n",
      "2.401310920715332\n",
      "2.422488212585449\n",
      "2.5118236541748047\n",
      "2.563178062438965\n",
      "2.512303352355957\n",
      "2.643165111541748\n",
      "2.461451292037964\n",
      "2.418225049972534\n",
      "2.366147756576538\n",
      "2.548638105392456\n",
      "2.3791234493255615\n",
      "2.3539481163024902\n",
      "2.5522522926330566\n",
      "2.5081634521484375\n",
      "2.5766823291778564\n",
      "2.5105953216552734\n",
      "2.477714776992798\n",
      "2.396306276321411\n",
      "2.5184426307678223\n",
      "2.532170295715332\n",
      "2.405414342880249\n",
      "2.533999443054199\n",
      "2.4678919315338135\n",
      "2.3936574459075928\n",
      "2.452810764312744\n",
      "2.6075315475463867\n",
      "2.4817678928375244\n",
      "2.5010366439819336\n",
      "2.4783968925476074\n",
      "2.464789390563965\n",
      "2.45584774017334\n",
      "2.4559123516082764\n",
      "2.5521960258483887\n",
      "2.433272123336792\n",
      "2.435739755630493\n",
      "2.5332069396972656\n",
      "2.548640489578247\n",
      "2.4680144786834717\n",
      "2.4484453201293945\n",
      "2.430847406387329\n",
      "2.4305837154388428\n",
      "2.544886589050293\n",
      "2.4019622802734375\n",
      "2.487354278564453\n",
      "2.449293613433838\n",
      "2.6143622398376465\n",
      "2.5499398708343506\n",
      "2.3242435455322266\n",
      "2.4661989212036133\n",
      "2.5731537342071533\n",
      "2.4568264484405518\n",
      "2.4860267639160156\n",
      "2.423532247543335\n",
      "2.4820022583007812\n",
      "2.5962259769439697\n",
      "2.5304718017578125\n",
      "2.4637844562530518\n",
      "2.4239280223846436\n",
      "2.532442569732666\n",
      "2.4451818466186523\n",
      "2.488274574279785\n",
      "2.4319372177124023\n",
      "2.479710578918457\n",
      "2.3928639888763428\n",
      "2.496950387954712\n",
      "2.486497163772583\n",
      "2.496795177459717\n",
      "2.479362726211548\n",
      "2.4245755672454834\n",
      "2.5198731422424316\n",
      "2.415803909301758\n",
      "2.489063262939453\n",
      "2.535548448562622\n",
      "2.628822088241577\n",
      "2.4496841430664062\n",
      "2.4107584953308105\n",
      "2.4267711639404297\n",
      "2.2928555011749268\n",
      "2.5343096256256104\n",
      "2.486342191696167\n",
      "2.4590556621551514\n",
      "2.470287561416626\n",
      "2.4727792739868164\n",
      "2.5571842193603516\n",
      "2.611060380935669\n",
      "2.661233901977539\n",
      "2.5214550495147705\n",
      "2.502197265625\n",
      "2.369310140609741\n",
      "2.384507417678833\n",
      "2.5885536670684814\n",
      "2.3979201316833496\n",
      "2.5303955078125\n",
      "2.531869411468506\n",
      "2.490328311920166\n",
      "2.4471805095672607\n",
      "2.4565651416778564\n",
      "2.401679039001465\n",
      "2.4612836837768555\n",
      "2.4223520755767822\n",
      "2.4638967514038086\n",
      "2.4222326278686523\n",
      "2.5040206909179688\n",
      "2.4598758220672607\n",
      "2.56708025932312\n",
      "2.422065019607544\n",
      "2.467352867126465\n",
      "2.459841728210449\n",
      "2.466254949569702\n",
      "2.4826557636260986\n",
      "2.549386739730835\n",
      "2.4043819904327393\n",
      "2.379359722137451\n",
      "2.5141701698303223\n",
      "2.4906654357910156\n",
      "2.4653961658477783\n",
      "2.517834424972534\n",
      "2.4889039993286133\n",
      "2.509159564971924\n",
      "2.5153613090515137\n",
      "2.5227701663970947\n",
      "2.4966509342193604\n",
      "2.438913106918335\n",
      "2.4295146465301514\n",
      "2.518017053604126\n",
      "2.4279098510742188\n",
      "2.357668399810791\n",
      "2.475228786468506\n",
      "2.5159528255462646\n",
      "2.5028843879699707\n",
      "2.6209049224853516\n",
      "2.460042715072632\n",
      "2.4809305667877197\n",
      "2.5875232219696045\n",
      "2.429534673690796\n",
      "2.4666337966918945\n",
      "2.4607324600219727\n",
      "2.518390655517578\n",
      "2.4924979209899902\n",
      "2.429649829864502\n",
      "2.513453245162964\n",
      "2.414252519607544\n",
      "2.4713199138641357\n",
      "2.6006972789764404\n",
      "2.356340169906616\n",
      "2.4260594844818115\n",
      "2.4570491313934326\n",
      "2.446265459060669\n",
      "2.5035858154296875\n",
      "2.5470669269561768\n",
      "2.388139247894287\n",
      "2.5353431701660156\n",
      "2.4359915256500244\n",
      "2.519045829772949\n",
      "2.5516817569732666\n",
      "2.3820085525512695\n",
      "2.416043996810913\n",
      "2.508086919784546\n",
      "2.4434356689453125\n",
      "2.377323627471924\n",
      "2.580857276916504\n",
      "2.447108030319214\n",
      "2.3525164127349854\n",
      "2.360590696334839\n",
      "2.56746768951416\n",
      "2.433384418487549\n",
      "2.4429798126220703\n",
      "2.440056800842285\n",
      "2.521169900894165\n",
      "2.509408712387085\n",
      "2.5578296184539795\n",
      "2.4373326301574707\n",
      "2.4334962368011475\n",
      "2.5510029792785645\n",
      "2.4194083213806152\n",
      "2.3011043071746826\n",
      "2.4850211143493652\n",
      "2.5890703201293945\n",
      "2.512293815612793\n",
      "2.384368658065796\n",
      "2.46616268157959\n",
      "2.535761594772339\n",
      "2.430065631866455\n",
      "2.511948347091675\n",
      "2.464322328567505\n",
      "2.540883779525757\n",
      "2.3923795223236084\n",
      "2.468109607696533\n",
      "2.4217679500579834\n",
      "2.651188373565674\n",
      "2.332737445831299\n",
      "2.481525421142578\n",
      "2.5964856147766113\n",
      "2.5107994079589844\n",
      "2.50985050201416\n",
      "2.4811105728149414\n",
      "2.351907968521118\n",
      "2.34269642829895\n",
      "2.492750644683838\n",
      "2.4670448303222656\n",
      "2.3948111534118652\n",
      "2.519631862640381\n",
      "2.633496046066284\n",
      "2.469825267791748\n",
      "2.4747958183288574\n",
      "2.5495004653930664\n",
      "2.5958406925201416\n",
      "2.5544774532318115\n",
      "2.4075047969818115\n",
      "2.4497828483581543\n",
      "2.4172112941741943\n",
      "2.393012046813965\n",
      "2.42714786529541\n",
      "2.446739673614502\n",
      "2.4410977363586426\n",
      "2.3953816890716553\n",
      "2.586582660675049\n",
      "2.4829702377319336\n",
      "2.4338042736053467\n",
      "2.415635347366333\n",
      "2.3895113468170166\n",
      "2.3943727016448975\n",
      "2.4539272785186768\n",
      "2.435447931289673\n",
      "2.4646685123443604\n",
      "2.4701690673828125\n",
      "2.5936341285705566\n",
      "2.6084487438201904\n",
      "2.5085182189941406\n",
      "2.4181931018829346\n",
      "2.490833282470703\n",
      "2.3586039543151855\n",
      "2.412437915802002\n",
      "2.4384758472442627\n",
      "2.357773542404175\n",
      "2.492587089538574\n",
      "2.5681393146514893\n",
      "2.566196918487549\n",
      "2.50864839553833\n",
      "2.4007034301757812\n",
      "2.5150063037872314\n",
      "2.4900825023651123\n",
      "2.5062644481658936\n",
      "2.479729175567627\n",
      "2.5453391075134277\n",
      "2.3817391395568848\n",
      "2.5040998458862305\n",
      "2.3680381774902344\n",
      "2.554025411605835\n",
      "2.4330365657806396\n",
      "2.357698440551758\n",
      "2.3862380981445312\n",
      "2.3791255950927734\n",
      "2.3546383380889893\n",
      "2.4124598503112793\n",
      "2.376551628112793\n",
      "2.5530200004577637\n",
      "2.437999725341797\n",
      "2.4996848106384277\n",
      "2.6336443424224854\n",
      "2.5273709297180176\n",
      "2.4727046489715576\n",
      "2.546937942504883\n",
      "2.4256107807159424\n",
      "2.526984453201294\n",
      "2.673290491104126\n",
      "2.462571382522583\n",
      "2.4789817333221436\n",
      "2.4171230792999268\n",
      "2.365725517272949\n",
      "2.611457347869873\n",
      "2.455233573913574\n",
      "2.631906032562256\n",
      "2.5226285457611084\n",
      "2.502318859100342\n",
      "2.494507074356079\n",
      "2.4745099544525146\n",
      "2.468287467956543\n",
      "2.509305953979492\n",
      "2.4940102100372314\n",
      "2.537396192550659\n",
      "2.4442291259765625\n",
      "2.42364501953125\n",
      "2.5189731121063232\n",
      "2.4385440349578857\n",
      "2.480504035949707\n",
      "2.5292224884033203\n",
      "2.5886099338531494\n",
      "2.3871238231658936\n",
      "2.5623254776000977\n",
      "2.4798083305358887\n",
      "2.4417171478271484\n",
      "2.436431646347046\n",
      "2.4187047481536865\n",
      "2.4262423515319824\n",
      "2.523346424102783\n",
      "2.404212236404419\n",
      "2.4720113277435303\n",
      "2.410050630569458\n",
      "2.3279802799224854\n",
      "2.482180118560791\n",
      "2.4695932865142822\n",
      "2.549804449081421\n",
      "2.4728667736053467\n",
      "2.460212230682373\n",
      "2.4694809913635254\n",
      "2.612445592880249\n",
      "2.3946759700775146\n",
      "2.4540257453918457\n",
      "2.4746670722961426\n",
      "2.377000331878662\n",
      "2.5599818229675293\n",
      "2.3278567790985107\n",
      "2.3935468196868896\n",
      "2.490116834640503\n",
      "2.4770941734313965\n",
      "2.3504250049591064\n",
      "2.4912354946136475\n",
      "2.490917205810547\n",
      "2.501221179962158\n",
      "2.4648194313049316\n",
      "2.452284812927246\n",
      "2.5242881774902344\n",
      "2.58187198638916\n",
      "2.5372703075408936\n",
      "2.4865574836730957\n",
      "2.2926437854766846\n",
      "2.4707443714141846\n",
      "2.471705913543701\n",
      "2.5921027660369873\n",
      "2.426762819290161\n",
      "2.666440010070801\n",
      "2.5318808555603027\n",
      "2.5970122814178467\n",
      "2.5655338764190674\n",
      "2.588890552520752\n",
      "2.521467924118042\n",
      "2.4217076301574707\n",
      "2.5602500438690186\n",
      "2.4500248432159424\n",
      "2.348120927810669\n",
      "2.4717559814453125\n",
      "2.4668946266174316\n",
      "2.48856782913208\n",
      "2.534289598464966\n",
      "2.5525474548339844\n",
      "2.5395114421844482\n",
      "2.5530474185943604\n",
      "2.3996455669403076\n",
      "2.3345487117767334\n",
      "2.2806577682495117\n",
      "2.5512402057647705\n",
      "2.3082330226898193\n",
      "2.4356329441070557\n",
      "2.445998430252075\n",
      "2.5056755542755127\n",
      "2.4029603004455566\n",
      "2.502978563308716\n",
      "2.488116502761841\n",
      "2.558718681335449\n",
      "2.484992504119873\n",
      "2.390584945678711\n",
      "2.3402695655822754\n",
      "2.4331977367401123\n",
      "2.4531190395355225\n",
      "2.475250005722046\n",
      "2.515401840209961\n",
      "2.3904597759246826\n",
      "2.519831657409668\n",
      "2.448789358139038\n",
      "2.3296761512756348\n",
      "2.395873546600342\n",
      "2.4358997344970703\n",
      "2.5244078636169434\n",
      "2.471581220626831\n",
      "2.5134456157684326\n",
      "2.4963738918304443\n",
      "2.4278383255004883\n",
      "2.4304940700531006\n",
      "2.3328747749328613\n",
      "2.595864772796631\n",
      "2.393937826156616\n",
      "2.5377426147460938\n",
      "2.519709348678589\n",
      "2.4468724727630615\n",
      "2.5091311931610107\n",
      "2.469045400619507\n",
      "2.514974594116211\n",
      "2.6035611629486084\n",
      "2.4715781211853027\n",
      "2.372040033340454\n",
      "2.510287284851074\n",
      "2.4758689403533936\n",
      "2.4429633617401123\n",
      "2.4545185565948486\n",
      "2.592590808868408\n",
      "2.579540729522705\n",
      "2.3722591400146484\n",
      "2.4275197982788086\n",
      "2.4770467281341553\n",
      "2.4838685989379883\n",
      "2.4950153827667236\n",
      "2.30100679397583\n",
      "2.5142414569854736\n",
      "2.436022996902466\n",
      "2.499303102493286\n",
      "2.453690528869629\n",
      "2.4114768505096436\n",
      "2.339668035507202\n",
      "2.497188091278076\n",
      "2.5046496391296387\n",
      "2.481743097305298\n",
      "2.437115430831909\n",
      "2.4951303005218506\n",
      "2.3483331203460693\n",
      "2.482792377471924\n",
      "2.4637529850006104\n",
      "2.5869789123535156\n",
      "2.616138219833374\n",
      "2.4231832027435303\n",
      "2.398574113845825\n",
      "2.560737133026123\n",
      "2.4967339038848877\n",
      "2.4492862224578857\n",
      "2.636242389678955\n",
      "2.4831244945526123\n",
      "2.366879463195801\n",
      "2.3925371170043945\n",
      "2.522832155227661\n",
      "2.529338836669922\n",
      "2.443366050720215\n",
      "2.4814202785491943\n",
      "2.433452844619751\n",
      "2.4616823196411133\n",
      "2.5012903213500977\n",
      "2.496443510055542\n",
      "2.7165722846984863\n",
      "2.571725368499756\n",
      "2.383269786834717\n",
      "2.5638720989227295\n",
      "2.4494075775146484\n",
      "2.571275472640991\n",
      "2.51733660697937\n",
      "2.323803424835205\n",
      "2.311614513397217\n",
      "2.529909133911133\n",
      "2.5258219242095947\n",
      "2.527496099472046\n",
      "2.5410048961639404\n",
      "2.4951045513153076\n",
      "2.4979076385498047\n",
      "2.520031452178955\n",
      "2.428462028503418\n",
      "2.5191800594329834\n",
      "2.431685447692871\n",
      "2.509674072265625\n",
      "2.642829656600952\n",
      "2.4111199378967285\n",
      "2.415479898452759\n",
      "2.474759101867676\n",
      "2.5538601875305176\n",
      "2.3268001079559326\n",
      "2.4024646282196045\n",
      "2.4941484928131104\n",
      "2.541201114654541\n",
      "2.5668246746063232\n",
      "2.530094623565674\n",
      "2.43837571144104\n",
      "2.5298523902893066\n",
      "2.464700222015381\n",
      "2.5236423015594482\n",
      "2.4628310203552246\n",
      "2.5177295207977295\n",
      "2.54028058052063\n",
      "2.5794711112976074\n",
      "2.3578169345855713\n",
      "2.4923579692840576\n",
      "2.4223074913024902\n",
      "2.521758556365967\n",
      "2.5327072143554688\n",
      "2.4201672077178955\n",
      "2.3683102130889893\n",
      "2.3821229934692383\n",
      "2.499208450317383\n",
      "2.5612051486968994\n",
      "2.3773205280303955\n",
      "2.4937477111816406\n",
      "2.3577818870544434\n",
      "2.5029213428497314\n",
      "2.416975736618042\n",
      "2.3880598545074463\n",
      "2.4154224395751953\n",
      "2.433500051498413\n",
      "2.4348549842834473\n",
      "2.5047295093536377\n",
      "2.391033411026001\n",
      "2.352919578552246\n",
      "2.4863712787628174\n",
      "2.478198528289795\n",
      "2.3390629291534424\n",
      "2.525174140930176\n",
      "2.5157275199890137\n",
      "2.4627127647399902\n",
      "2.469439744949341\n",
      "2.5107312202453613\n",
      "2.4509682655334473\n",
      "2.5178258419036865\n",
      "2.639096736907959\n",
      "2.4646286964416504\n",
      "2.52607798576355\n",
      "2.472562551498413\n",
      "2.3976707458496094\n",
      "2.4785733222961426\n",
      "2.5803964138031006\n",
      "2.5472586154937744\n",
      "2.368293285369873\n",
      "2.540886640548706\n",
      "2.483452558517456\n",
      "2.453824758529663\n",
      "2.374298572540283\n",
      "2.4482221603393555\n",
      "2.400820255279541\n",
      "2.501995801925659\n",
      "2.513653039932251\n",
      "2.488372325897217\n",
      "2.5865466594696045\n",
      "2.409010648727417\n",
      "2.6069867610931396\n",
      "2.4976577758789062\n",
      "2.4693078994750977\n",
      "2.508492946624756\n",
      "2.4925179481506348\n",
      "2.495504379272461\n",
      "2.3125815391540527\n",
      "2.504361867904663\n",
      "2.4770455360412598\n",
      "2.435739755630493\n",
      "2.347456932067871\n",
      "2.5206477642059326\n",
      "2.33502459526062\n",
      "2.371596574783325\n",
      "2.406014919281006\n",
      "2.4799513816833496\n",
      "2.47055721282959\n",
      "2.4659440517425537\n",
      "2.425401210784912\n",
      "2.517086982727051\n",
      "2.5176990032196045\n",
      "2.6197824478149414\n",
      "2.5508131980895996\n",
      "2.4446403980255127\n",
      "2.4624078273773193\n",
      "2.423095464706421\n",
      "2.457240581512451\n",
      "2.472754955291748\n",
      "2.5048182010650635\n",
      "2.527658700942993\n",
      "2.541421413421631\n",
      "2.33278226852417\n",
      "2.382591724395752\n",
      "2.5749475955963135\n",
      "2.504554510116577\n",
      "2.4951012134552\n",
      "2.493685722351074\n",
      "2.333033561706543\n",
      "2.279315710067749\n",
      "2.337491512298584\n",
      "2.4593541622161865\n",
      "2.4578797817230225\n",
      "2.53662371635437\n",
      "2.5206518173217773\n",
      "2.44942045211792\n",
      "2.507327079772949\n",
      "2.5187935829162598\n",
      "2.4642529487609863\n",
      "2.4483110904693604\n",
      "2.446204662322998\n",
      "2.290771484375\n",
      "2.5170674324035645\n",
      "2.4327027797698975\n",
      "2.500682830810547\n",
      "2.645885467529297\n",
      "2.5551700592041016\n",
      "2.3964598178863525\n",
      "2.4573638439178467\n",
      "2.378467321395874\n",
      "2.426091432571411\n",
      "2.4018197059631348\n",
      "2.364898443222046\n",
      "2.5054123401641846\n",
      "2.5528879165649414\n",
      "2.419327974319458\n",
      "2.4959986209869385\n",
      "2.5832407474517822\n",
      "2.460677146911621\n",
      "2.426988363265991\n",
      "2.3242180347442627\n",
      "2.3874762058258057\n",
      "2.497731924057007\n",
      "2.4762144088745117\n",
      "2.395946502685547\n",
      "2.478405475616455\n",
      "2.4560205936431885\n",
      "2.433258295059204\n",
      "2.5600056648254395\n",
      "2.4140217304229736\n",
      "2.364351987838745\n",
      "2.7158501148223877\n",
      "2.395019054412842\n",
      "2.584773302078247\n",
      "2.554492950439453\n",
      "2.4676406383514404\n",
      "2.4352073669433594\n",
      "2.549168109893799\n",
      "2.4938950538635254\n",
      "2.4832091331481934\n",
      "2.452646255493164\n",
      "2.470398187637329\n",
      "2.504966974258423\n",
      "2.4303243160247803\n",
      "2.384361505508423\n",
      "2.528834342956543\n",
      "2.410586357116699\n",
      "2.6345431804656982\n",
      "2.4846551418304443\n",
      "2.560166597366333\n",
      "2.469137191772461\n",
      "2.4243879318237305\n",
      "2.4334938526153564\n",
      "2.5048129558563232\n",
      "2.4994382858276367\n",
      "2.589604139328003\n",
      "2.4804999828338623\n",
      "2.5319643020629883\n",
      "2.4994254112243652\n",
      "2.4928741455078125\n",
      "2.4509336948394775\n",
      "2.4188828468322754\n",
      "2.396285057067871\n",
      "2.370526075363159\n",
      "2.50464129447937\n",
      "2.624826669692993\n",
      "2.5074522495269775\n",
      "2.4196670055389404\n",
      "2.3600480556488037\n",
      "2.480372905731201\n",
      "2.460462808609009\n",
      "2.5052132606506348\n",
      "2.475001335144043\n",
      "2.4244606494903564\n",
      "2.5754783153533936\n",
      "2.476287841796875\n",
      "2.449192762374878\n",
      "2.602118968963623\n",
      "2.467129707336426\n",
      "2.5220067501068115\n",
      "2.4844863414764404\n",
      "2.4694204330444336\n",
      "2.3846776485443115\n",
      "2.425691843032837\n",
      "2.4436144828796387\n",
      "2.378309488296509\n",
      "2.5270564556121826\n",
      "2.518961191177368\n",
      "2.383112668991089\n",
      "2.527486801147461\n",
      "2.565892219543457\n",
      "2.403371572494507\n",
      "2.477360725402832\n",
      "2.462019681930542\n",
      "2.5161876678466797\n",
      "2.4252712726593018\n",
      "2.4585723876953125\n",
      "2.4383411407470703\n",
      "2.4678597450256348\n",
      "2.494206428527832\n",
      "2.503410816192627\n",
      "2.3382627964019775\n",
      "2.4007623195648193\n",
      "2.5713729858398438\n",
      "2.5012104511260986\n",
      "2.4767043590545654\n",
      "2.4663245677948\n",
      "2.435671091079712\n",
      "2.466500997543335\n",
      "2.430983543395996\n",
      "2.430560827255249\n",
      "2.39643931388855\n",
      "2.3854968547821045\n",
      "2.5189366340637207\n",
      "2.4216768741607666\n",
      "2.50114107131958\n",
      "2.4240057468414307\n",
      "2.485196113586426\n",
      "2.5082714557647705\n",
      "2.434892177581787\n",
      "2.3849000930786133\n",
      "2.4406158924102783\n",
      "2.5042002201080322\n",
      "2.466092586517334\n",
      "2.5875980854034424\n",
      "2.5262551307678223\n",
      "2.5280048847198486\n",
      "2.497025966644287\n",
      "2.516801357269287\n",
      "2.4408442974090576\n",
      "2.5171351432800293\n",
      "2.3655221462249756\n",
      "2.4383320808410645\n",
      "2.4866178035736084\n",
      "2.3041763305664062\n",
      "2.593493938446045\n",
      "2.546113967895508\n",
      "2.4596829414367676\n",
      "2.4805819988250732\n",
      "2.497359275817871\n",
      "2.3173816204071045\n",
      "2.3851754665374756\n",
      "2.46618390083313\n",
      "2.488642454147339\n",
      "2.4352569580078125\n",
      "2.3182976245880127\n",
      "2.457836389541626\n",
      "2.46838641166687\n",
      "2.4010274410247803\n",
      "2.4553093910217285\n",
      "2.442467212677002\n",
      "2.438999891281128\n",
      "2.465398073196411\n",
      "2.55945086479187\n",
      "2.5409600734710693\n",
      "2.4549310207366943\n",
      "2.4558260440826416\n",
      "2.4020345211029053\n",
      "2.3962557315826416\n",
      "2.424142837524414\n",
      "2.4988021850585938\n",
      "2.3757572174072266\n",
      "2.480090856552124\n",
      "2.582960844039917\n",
      "2.303041934967041\n",
      "2.542163372039795\n",
      "2.4776668548583984\n",
      "2.540764093399048\n",
      "2.4084553718566895\n",
      "2.3599660396575928\n",
      "2.5426628589630127\n",
      "2.454864740371704\n",
      "2.408977508544922\n",
      "2.446181297302246\n",
      "2.5157670974731445\n",
      "2.5246756076812744\n",
      "2.401059150695801\n",
      "2.437267303466797\n",
      "2.531571626663208\n",
      "2.444340467453003\n",
      "2.35909366607666\n",
      "2.4346158504486084\n",
      "2.4445865154266357\n",
      "2.4712464809417725\n",
      "2.37713360786438\n",
      "2.3778440952301025\n",
      "2.4647557735443115\n",
      "2.4912288188934326\n",
      "2.514925003051758\n",
      "2.5436198711395264\n",
      "2.4876222610473633\n",
      "2.3478963375091553\n",
      "2.4842722415924072\n",
      "2.4293699264526367\n",
      "2.45811128616333\n",
      "2.5002517700195312\n",
      "2.564931631088257\n",
      "2.534280776977539\n",
      "2.461089611053467\n",
      "2.428900718688965\n",
      "2.390939474105835\n",
      "2.4201927185058594\n",
      "2.38352108001709\n",
      "2.4261038303375244\n",
      "2.5308125019073486\n",
      "2.4223251342773438\n",
      "2.5192203521728516\n",
      "2.492628574371338\n",
      "2.5524556636810303\n",
      "2.605684280395508\n",
      "2.3955461978912354\n",
      "2.446958541870117\n",
      "2.5777857303619385\n",
      "2.3390448093414307\n",
      "2.2566146850585938\n",
      "2.424039363861084\n",
      "2.484607458114624\n",
      "2.451871871948242\n",
      "2.3275649547576904\n",
      "2.5061676502227783\n",
      "2.3239150047302246\n",
      "2.639456033706665\n",
      "2.338383197784424\n",
      "2.500235080718994\n",
      "2.421086072921753\n",
      "2.511343479156494\n",
      "2.5108494758605957\n",
      "2.582042932510376\n",
      "2.4143919944763184\n",
      "2.3338544368743896\n",
      "2.375286817550659\n",
      "2.4885823726654053\n",
      "2.4631874561309814\n",
      "2.6277830600738525\n",
      "2.4135830402374268\n",
      "2.5385890007019043\n",
      "2.51269793510437\n",
      "2.3726723194122314\n",
      "2.367588520050049\n",
      "2.5161314010620117\n",
      "2.4352264404296875\n",
      "2.5534896850585938\n",
      "2.38150691986084\n",
      "2.506920099258423\n",
      "2.399897336959839\n",
      "2.4405834674835205\n",
      "2.5733118057250977\n",
      "2.4158575534820557\n",
      "2.4822020530700684\n",
      "2.3906030654907227\n",
      "2.500605821609497\n",
      "2.5055713653564453\n",
      "2.4396026134490967\n",
      "2.5249857902526855\n",
      "2.4288673400878906\n",
      "2.424873113632202\n",
      "2.4074835777282715\n",
      "2.5333385467529297\n",
      "2.474480390548706\n",
      "2.5000267028808594\n",
      "2.421769380569458\n",
      "2.5397608280181885\n",
      "2.515226364135742\n",
      "2.473156213760376\n",
      "2.3616151809692383\n",
      "2.401735782623291\n",
      "2.4653165340423584\n",
      "2.5162577629089355\n",
      "2.475858688354492\n",
      "2.448155403137207\n",
      "2.5221762657165527\n",
      "2.454538583755493\n",
      "2.526491165161133\n",
      "2.3995230197906494\n",
      "2.496222496032715\n",
      "2.59572696685791\n",
      "2.472541570663452\n",
      "2.5020902156829834\n",
      "2.4185850620269775\n",
      "2.611470937728882\n",
      "2.517584800720215\n",
      "2.4096431732177734\n",
      "2.5920557975769043\n",
      "2.520730972290039\n",
      "2.4921278953552246\n",
      "2.4224696159362793\n",
      "2.431995153427124\n",
      "2.451463460922241\n",
      "2.3055050373077393\n",
      "2.683351993560791\n",
      "2.4212660789489746\n",
      "2.395425319671631\n",
      "2.4609780311584473\n",
      "2.420001745223999\n",
      "2.465285539627075\n",
      "2.4390673637390137\n",
      "2.4504148960113525\n",
      "2.5813989639282227\n",
      "2.427208662033081\n",
      "2.4979090690612793\n",
      "2.500617742538452\n",
      "2.4017415046691895\n",
      "2.439757823944092\n",
      "2.394256830215454\n",
      "2.5428755283355713\n",
      "2.411762237548828\n",
      "2.52864146232605\n",
      "2.5508100986480713\n",
      "2.3682327270507812\n",
      "2.379472255706787\n",
      "2.4444878101348877\n",
      "2.527822971343994\n",
      "2.510538101196289\n",
      "2.4614856243133545\n",
      "2.5283381938934326\n",
      "2.4507203102111816\n",
      "2.45870041847229\n",
      "2.587428569793701\n",
      "2.552435874938965\n",
      "2.4724485874176025\n",
      "2.54148006439209\n",
      "2.3851444721221924\n",
      "2.4853522777557373\n",
      "2.367841958999634\n",
      "2.5496273040771484\n",
      "2.463714838027954\n",
      "2.3904457092285156\n",
      "2.4913320541381836\n",
      "2.4102020263671875\n",
      "2.4508018493652344\n",
      "2.482332229614258\n",
      "2.4792163372039795\n",
      "2.5297584533691406\n",
      "2.5110044479370117\n",
      "2.4615278244018555\n",
      "2.593449592590332\n",
      "2.528693437576294\n",
      "2.51619291305542\n",
      "2.426226854324341\n",
      "2.4722142219543457\n",
      "2.444833993911743\n",
      "2.4705231189727783\n",
      "2.676896095275879\n",
      "2.460420608520508\n",
      "2.4327914714813232\n",
      "2.5289206504821777\n",
      "2.447835683822632\n",
      "2.5072011947631836\n",
      "2.564800500869751\n",
      "2.449441909790039\n",
      "2.5011608600616455\n",
      "2.286189556121826\n",
      "2.567312717437744\n",
      "2.464630365371704\n",
      "2.362456798553467\n",
      "2.500434398651123\n",
      "2.4962239265441895\n",
      "2.6204066276550293\n",
      "2.520908832550049\n",
      "2.5062379837036133\n",
      "2.474113941192627\n",
      "2.4401986598968506\n",
      "2.527327060699463\n",
      "2.4659268856048584\n",
      "2.4850034713745117\n",
      "2.485588312149048\n",
      "2.314758539199829\n",
      "2.3976495265960693\n",
      "2.5371508598327637\n",
      "2.579127311706543\n",
      "2.527470111846924\n",
      "2.421058177947998\n",
      "2.410107374191284\n",
      "2.3266866207122803\n",
      "2.548621892929077\n",
      "2.4660041332244873\n",
      "2.4263696670532227\n",
      "2.5370986461639404\n",
      "2.5932772159576416\n",
      "2.481027364730835\n",
      "2.4209213256835938\n",
      "2.2931172847747803\n",
      "2.4236278533935547\n",
      "2.363323211669922\n",
      "2.570427417755127\n",
      "2.52431583404541\n",
      "2.355342388153076\n",
      "2.460766553878784\n",
      "2.478396415710449\n",
      "2.4701178073883057\n",
      "2.547192335128784\n",
      "2.5054569244384766\n",
      "2.4357388019561768\n",
      "2.4106667041778564\n",
      "2.4388060569763184\n",
      "2.4774818420410156\n",
      "2.5012545585632324\n",
      "2.411144495010376\n",
      "2.4222640991210938\n",
      "2.4842238426208496\n",
      "2.36909818649292\n",
      "2.4676218032836914\n",
      "2.415470838546753\n",
      "2.4164352416992188\n",
      "2.501776933670044\n",
      "2.580798625946045\n",
      "2.5116472244262695\n",
      "2.5828542709350586\n",
      "2.619673728942871\n",
      "2.4656145572662354\n",
      "2.4501657485961914\n",
      "2.426405191421509\n",
      "2.4743313789367676\n",
      "2.437800407409668\n",
      "2.596466302871704\n",
      "2.5312089920043945\n",
      "2.446767807006836\n",
      "2.4693570137023926\n",
      "2.339200735092163\n",
      "2.4625051021575928\n",
      "2.4372830390930176\n",
      "2.4258322715759277\n",
      "2.541043758392334\n",
      "2.4805333614349365\n",
      "2.445202350616455\n",
      "2.5237696170806885\n",
      "2.4723479747772217\n",
      "2.4700279235839844\n",
      "2.4951748847961426\n",
      "2.4556422233581543\n",
      "2.6127240657806396\n",
      "2.5782995223999023\n",
      "2.5375940799713135\n",
      "2.4299886226654053\n",
      "2.5209946632385254\n",
      "2.469538688659668\n",
      "2.47497820854187\n",
      "2.5710372924804688\n",
      "2.5173611640930176\n",
      "2.3618228435516357\n",
      "2.411789655685425\n",
      "2.471252918243408\n",
      "2.4770569801330566\n",
      "2.565352439880371\n",
      "2.420025110244751\n",
      "2.4740421772003174\n",
      "2.3579256534576416\n",
      "2.43548846244812\n",
      "2.405951976776123\n",
      "2.4237022399902344\n",
      "2.5644280910491943\n",
      "2.5348610877990723\n",
      "2.3841090202331543\n",
      "2.3293039798736572\n",
      "2.4575603008270264\n",
      "2.4031262397766113\n",
      "2.474686861038208\n",
      "2.4844024181365967\n",
      "2.456023693084717\n",
      "2.4039511680603027\n",
      "2.485267162322998\n",
      "2.3827171325683594\n",
      "2.439936399459839\n",
      "2.5236685276031494\n",
      "2.4518802165985107\n",
      "2.4722673892974854\n",
      "2.375239133834839\n",
      "2.4397575855255127\n",
      "2.389089822769165\n",
      "2.524289608001709\n",
      "2.5397841930389404\n",
      "2.319157838821411\n",
      "2.246370553970337\n",
      "2.516845703125\n",
      "2.3397226333618164\n",
      "2.4229044914245605\n",
      "2.4091272354125977\n",
      "2.3872668743133545\n",
      "2.4320013523101807\n",
      "2.6239705085754395\n",
      "2.3500092029571533\n",
      "2.5127322673797607\n",
      "2.440554618835449\n",
      "2.4217259883880615\n",
      "2.3504722118377686\n",
      "2.550076961517334\n",
      "2.3936357498168945\n",
      "2.377945899963379\n",
      "2.5057449340820312\n",
      "2.5087642669677734\n",
      "2.5032458305358887\n",
      "2.304725170135498\n",
      "2.439441204071045\n",
      "2.4888598918914795\n",
      "2.4073634147644043\n",
      "2.550342559814453\n",
      "2.535191535949707\n",
      "2.566262722015381\n",
      "2.4331376552581787\n",
      "2.5098021030426025\n",
      "2.502709150314331\n",
      "2.5117483139038086\n",
      "2.4776296615600586\n",
      "2.408545970916748\n",
      "2.5815742015838623\n",
      "2.4862260818481445\n",
      "2.427507162094116\n",
      "2.4545090198516846\n",
      "2.52001953125\n",
      "2.56569504737854\n",
      "2.419248580932617\n",
      "2.3202743530273438\n",
      "2.481881618499756\n",
      "2.525519609451294\n",
      "2.4268887042999268\n",
      "2.5347115993499756\n",
      "2.6021640300750732\n",
      "2.4828953742980957\n",
      "2.2699735164642334\n",
      "2.4045071601867676\n",
      "2.427269458770752\n",
      "2.5188848972320557\n",
      "2.4898681640625\n",
      "2.4198861122131348\n",
      "2.510833263397217\n",
      "2.51446795463562\n",
      "2.4604718685150146\n",
      "2.3748178482055664\n",
      "2.4156906604766846\n",
      "2.346759080886841\n",
      "2.3850443363189697\n",
      "2.2790162563323975\n",
      "2.4628124237060547\n",
      "2.4514551162719727\n",
      "2.4853384494781494\n",
      "2.5206539630889893\n",
      "2.5466482639312744\n",
      "2.3196067810058594\n",
      "2.387830972671509\n",
      "2.54522705078125\n",
      "2.52213978767395\n",
      "2.4582901000976562\n",
      "2.5228474140167236\n",
      "2.421140432357788\n",
      "2.463083028793335\n",
      "2.5173661708831787\n",
      "2.4956367015838623\n",
      "2.3804538249969482\n",
      "2.534862518310547\n",
      "2.403907060623169\n",
      "2.488403081893921\n",
      "2.4353621006011963\n",
      "2.4110352993011475\n",
      "2.5455360412597656\n",
      "2.4923648834228516\n",
      "2.4016847610473633\n",
      "2.5603959560394287\n",
      "2.434741973876953\n",
      "2.5127601623535156\n",
      "2.3864731788635254\n",
      "2.481934070587158\n",
      "2.513730764389038\n",
      "2.516925096511841\n",
      "2.389972448348999\n",
      "2.4422199726104736\n",
      "2.3478939533233643\n",
      "2.5343501567840576\n",
      "2.307976722717285\n",
      "2.4693450927734375\n",
      "2.4283130168914795\n",
      "2.400911569595337\n",
      "2.490056276321411\n",
      "2.370640516281128\n",
      "2.42765736579895\n",
      "2.5583221912384033\n",
      "2.4879798889160156\n",
      "2.5610909461975098\n",
      "2.368846893310547\n",
      "2.5173566341400146\n",
      "2.539661407470703\n",
      "2.4828460216522217\n",
      "2.4351727962493896\n",
      "2.4726197719573975\n",
      "2.4866392612457275\n",
      "2.577064275741577\n",
      "2.510425329208374\n",
      "2.4233341217041016\n",
      "2.5271999835968018\n",
      "2.550204038619995\n",
      "2.4572901725769043\n",
      "2.61688232421875\n",
      "2.5062668323516846\n",
      "2.467247486114502\n",
      "2.4154539108276367\n",
      "2.3651726245880127\n",
      "2.2695281505584717\n",
      "2.418318510055542\n",
      "2.5287747383117676\n",
      "2.594944953918457\n",
      "2.419074296951294\n",
      "2.4663569927215576\n",
      "2.52870774269104\n",
      "2.3587493896484375\n",
      "2.4124643802642822\n",
      "2.4342598915100098\n",
      "2.4473817348480225\n",
      "2.5807909965515137\n",
      "2.402632236480713\n",
      "2.5100314617156982\n",
      "2.497260570526123\n",
      "2.576352834701538\n",
      "2.3548190593719482\n",
      "2.6972365379333496\n",
      "2.3335230350494385\n",
      "2.452712297439575\n",
      "2.2251503467559814\n",
      "2.4138119220733643\n",
      "2.3398594856262207\n",
      "2.500175714492798\n",
      "2.4598703384399414\n",
      "2.349609136581421\n",
      "2.4777088165283203\n",
      "2.489274024963379\n",
      "2.3738317489624023\n",
      "2.2334861755371094\n",
      "2.543285846710205\n",
      "2.510528326034546\n",
      "2.440768241882324\n",
      "2.6920745372772217\n",
      "2.5229737758636475\n",
      "2.532925605773926\n",
      "2.542330265045166\n",
      "2.486787796020508\n",
      "2.5006651878356934\n",
      "2.446606159210205\n",
      "2.4813945293426514\n",
      "2.391968011856079\n",
      "2.4983925819396973\n",
      "2.434412956237793\n",
      "2.534532070159912\n",
      "2.4226579666137695\n",
      "2.5107076168060303\n",
      "2.4197516441345215\n",
      "2.4615345001220703\n",
      "2.511033773422241\n",
      "2.4497854709625244\n",
      "2.5214457511901855\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m logits, loss = model(xb, yb)\n\u001b[32m      7\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m optimizer.step()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sshuser\\MakeMore\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sshuser\\MakeMore\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sshuser\\MakeMore\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for step in range(1000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 2, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 3, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 4, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 5, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 6, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 7, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 8, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 9, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 10, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 11, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 12, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 13, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 14, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 15, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 16, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 17, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 18, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 19, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 20, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 21, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 22, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 23, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 24, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 25, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 26, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 27, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 28, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 29, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 30, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 31, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 32, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 33, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 34, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 35, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 36, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 37, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 38, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 39, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 40, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 41, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 42, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 43, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 44, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 45, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 46, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 47, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 48, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 49, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 50, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 51, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 52, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 53, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 54, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 55, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 56, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 57, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 58, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 59, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 60, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 61, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 62, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 63, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 64, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 65, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 66, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 67, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 68, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 69, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 70, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 71, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 72, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 73, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 74, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 75, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 76, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 77, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 78, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 79, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 80, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 81, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 82, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 83, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 84, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 85, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 86, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 87, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 88, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 89, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 90, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 91, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 92, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 93, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 94, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 95, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 96, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 97, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 98, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 99, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 100, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 101, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 102, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 103, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 104, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 105, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 106, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 107, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 108, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 109, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 110, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 111, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 112, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 113, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 114, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 115, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 116, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 117, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 118, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 119, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 120, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 121, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 122, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 123, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 124, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 125, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 126, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 127, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 128, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 129, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 130, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 131, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 132, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 133, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 134, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 135, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 136, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 137, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 138, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 139, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 140, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 141, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 142, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 143, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 144, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 145, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 146, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 147, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 148, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 149, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 150, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 151, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 152, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 153, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 154, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 155, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 156, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 157, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 158, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 159, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 160, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 161, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 162, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 163, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 164, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 165, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 166, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 167, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 168, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 169, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 170, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 171, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 172, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 173, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 174, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 175, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 176, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 177, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 178, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 179, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 180, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 181, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 182, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 183, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 184, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 185, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 186, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 187, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 188, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 189, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 190, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 191, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 192, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 193, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 194, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 195, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 196, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 197, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 198, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 199, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 200, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 201, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 202, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 203, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 204, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 205, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 206, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 207, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 208, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 209, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 210, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 211, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 212, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 213, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 214, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 215, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 216, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 217, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 218, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 219, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 220, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 221, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 222, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 223, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 224, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 225, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 226, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 227, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 228, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 229, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 230, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 231, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 232, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 233, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 234, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 235, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 236, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 237, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 238, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 239, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 240, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 241, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 242, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 243, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 244, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 245, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 246, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 247, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 248, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 249, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 250, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 251, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 252, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 253, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 254, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 255, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 256, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 257, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 258, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 259, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 260, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 261, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 262, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 263, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 264, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 265, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 266, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 267, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 268, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 269, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 270, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 271, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 272, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 273, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 274, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 275, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 276, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 277, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 278, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 279, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 280, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 281, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 282, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 283, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 284, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 285, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 286, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 287, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 288, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 289, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 290, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 291, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 292, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 293, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 294, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 295, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 296, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 297, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 298, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 299, 65])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 300, 65])\n",
      "torch.Size([1, 65])\n",
      "\n",
      "ORThay garomy CHA hthe IZE t ce'swilisteat\n",
      "HERI w bake he, sthy\n",
      "S:\n",
      "CIf ditomaisonute d whyow byoker twiartofre.\n",
      "HE:\n",
      "nse,\n",
      "CII'l t Sthenearon:\n",
      "S:\n",
      "\n",
      "Anomowhicthet,\n",
      "ICEven hilichos thton.\n",
      "AMat y thiseefos I'scostr mee sprans id\n",
      "TAMisheedeatof s hilowiler anes VOfrd f aiorthranins f tur;\n",
      "tes thy, cit.\n",
      "Sos\n"
     ]
    }
   ],
   "source": [
    "start_gen(model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ecee98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.0418, -0.2516,  0.8599],\n",
       "         [-1.3847, -0.8712, -0.2234,  ...,  1.8446, -1.1845,  1.3835],\n",
       "         [ 1.4451,  0.8564,  2.2181,  ..., -0.8278,  1.3347,  0.4835],\n",
       "         ...,\n",
       "         [-1.9006,  0.2286,  0.0249,  ..., -0.5558,  0.7043,  0.7099],\n",
       "         [ 1.7744, -0.9216,  0.9624,  ..., -0.5003,  1.0350,  1.6896],\n",
       "         [-0.0045,  1.6668,  0.1539,  ...,  0.5655,  0.5058,  0.2225]],\n",
       "\n",
       "        [[-0.6855,  0.5636, -1.5072,  ...,  1.1566,  0.2691, -0.0366],\n",
       "         [ 0.9733, -1.0151, -0.5419,  ..., -0.0553,  1.2049, -0.9825],\n",
       "         [ 0.4334, -0.7172,  1.0554,  ..., -0.6766, -0.5730, -0.3303],\n",
       "         ...,\n",
       "         [ 0.6839, -1.3246, -0.5161,  ...,  1.1895,  0.7607, -0.7463],\n",
       "         [-1.3839,  0.4869, -1.0020,  ...,  1.9535,  2.0487, -1.0880],\n",
       "         [ 1.6217,  0.8513, -0.4005,  ...,  0.4232, -0.3389,  0.5180]],\n",
       "\n",
       "        [[-1.3638,  0.1930, -0.6103,  ...,  0.6110,  1.2208, -0.6076],\n",
       "         [-1.7376, -0.1254, -1.3658,  ..., -0.6035, -0.1743,  0.6092],\n",
       "         [-0.8032, -1.1209,  0.1956,  ...,  0.1598,  1.7698,  0.6268],\n",
       "         ...,\n",
       "         [ 2.1296, -1.5181,  0.1387,  ...,  3.0250,  1.3463,  0.8556],\n",
       "         [ 0.3220,  0.4461,  1.5230,  ..., -1.4591, -1.4937, -0.2214],\n",
       "         [ 0.2252, -0.0772,  0.9857,  ..., -1.6034, -0.4298,  0.5762]],\n",
       "\n",
       "        [[ 0.3444, -3.1016, -1.4587,  ...,  1.4162,  0.6834, -0.1383],\n",
       "         [ 0.9213,  0.5282, -0.0082,  ...,  2.1477, -0.6604,  0.1135],\n",
       "         [-0.2206,  0.7118,  0.3416,  ...,  1.1383, -0.2505,  1.6705],\n",
       "         ...,\n",
       "         [ 0.0518, -0.3285, -2.2472,  ...,  1.4557, -0.3461, -0.2634],\n",
       "         [-0.4477, -0.7288, -0.1607,  ...,  0.5405,  0.4351, -2.2717],\n",
       "         [-0.1339, -0.0586,  0.1257,  ...,  1.1085,  0.5544,  1.5818]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "B,T,C = 4,8,32\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80290394",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = xprev.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de0fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9269e+00,  1.4873e+00,  9.0072e-01,  ...,  4.1759e-02,\n",
       "          -2.5158e-01,  8.5986e-01],\n",
       "         [ 2.7112e-01,  3.0802e-01,  3.3868e-01,  ...,  9.4318e-01,\n",
       "          -7.1806e-01,  1.1217e+00],\n",
       "         [ 6.6246e-01,  4.9082e-01,  9.6514e-01,  ...,  3.5286e-01,\n",
       "          -3.3803e-02,  9.0898e-01],\n",
       "         ...,\n",
       "         [-8.1927e-02,  5.3348e-01,  3.7808e-01,  ...,  7.0856e-02,\n",
       "           2.1229e-02,  7.7676e-01],\n",
       "         [ 1.8326e-01,  3.2562e-01,  4.6156e-01,  ..., -1.0739e-02,\n",
       "           1.6605e-01,  9.0717e-01],\n",
       "         [ 1.5979e-01,  4.9327e-01,  4.2310e-01,  ...,  6.1292e-02,\n",
       "           2.0852e-01,  8.2158e-01]],\n",
       "\n",
       "        [[-6.8548e-01,  5.6356e-01, -1.5072e+00,  ...,  1.1566e+00,\n",
       "           2.6905e-01, -3.6629e-02],\n",
       "         [ 1.4391e-01, -2.2576e-01, -1.0245e+00,  ...,  5.5064e-01,\n",
       "           7.3695e-01, -5.0955e-01],\n",
       "         [ 2.4042e-01, -3.8957e-01, -3.3124e-01,  ...,  1.4155e-01,\n",
       "           3.0030e-01, -4.4981e-01],\n",
       "         ...,\n",
       "         [ 2.1293e-01, -3.6539e-01, -2.7192e-01,  ...,  3.6531e-01,\n",
       "           1.6305e-01, -2.7973e-01],\n",
       "         [-1.5190e-02, -2.4364e-01, -3.7622e-01,  ...,  5.9219e-01,\n",
       "           4.3243e-01, -3.9521e-01],\n",
       "         [ 1.8942e-01, -1.0677e-01, -3.7925e-01,  ...,  5.7106e-01,\n",
       "           3.3601e-01, -2.8106e-01]],\n",
       "\n",
       "        [[-1.3638e+00,  1.9296e-01, -6.1033e-01,  ...,  6.1104e-01,\n",
       "           1.2208e+00, -6.0764e-01],\n",
       "         [-1.5507e+00,  3.3803e-02, -9.8807e-01,  ...,  3.7525e-03,\n",
       "           5.2329e-01,  7.8112e-04],\n",
       "         [-1.3015e+00, -3.5110e-01, -5.9350e-01,  ...,  5.5765e-02,\n",
       "           9.3879e-01,  2.0946e-01],\n",
       "         ...,\n",
       "         [-8.2122e-01, -4.7327e-02, -1.0806e-01,  ...,  9.1270e-01,\n",
       "           7.4220e-01,  5.3706e-02],\n",
       "         [-6.5790e-01,  2.3156e-02,  1.2494e-01,  ...,  5.7388e-01,\n",
       "           4.2279e-01,  1.4407e-02],\n",
       "         [-5.4751e-01,  1.0606e-02,  2.3254e-01,  ...,  3.0171e-01,\n",
       "           3.1621e-01,  8.4626e-02]],\n",
       "\n",
       "        [[ 3.4436e-01, -3.1016e+00, -1.4587e+00,  ...,  1.4162e+00,\n",
       "           6.8340e-01, -1.3825e-01],\n",
       "         [ 6.3283e-01, -1.2867e+00, -7.3348e-01,  ...,  1.7820e+00,\n",
       "           1.1492e-02, -1.2363e-02],\n",
       "         [ 3.4837e-01, -6.2052e-01, -3.7512e-01,  ...,  1.5674e+00,\n",
       "          -7.5846e-02,  5.4858e-01],\n",
       "         ...,\n",
       "         [-2.6577e-02, -7.8914e-01, -6.7926e-01,  ...,  1.1730e+00,\n",
       "          -2.1071e-01, -6.3257e-02],\n",
       "         [-8.6737e-02, -7.8053e-01, -6.0517e-01,  ...,  1.0826e+00,\n",
       "          -1.1846e-01, -3.7875e-01],\n",
       "         [-9.2628e-02, -6.9028e-01, -5.1381e-01,  ...,  1.0859e+00,\n",
       "          -3.4349e-02, -1.3369e-01]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9b0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei/wei.sum(1,keepdim=True)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) --> (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a/a.sum(1,keepdim=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bac663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 7.],\n",
       "        [6., 4.],\n",
       "        [6., 5.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8339268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 7.0000],\n",
       "        [4.0000, 5.5000],\n",
       "        [4.6667, 5.3333]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a @ b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fdb8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 16]), torch.Size([4, 8, 16]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B,T,C = 4, 8 ,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) #(32,16)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (4,8,32) @ (32, 16)\n",
    "q = query(x)\n",
    "\n",
    "k.shape, q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504949f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.083, -0.293, -0.255, -0.014, -0.274,  0.068,  0.034, -0.212],\n",
       "         [-0.165,  0.197, -0.318,  0.421,  0.029,  0.136,  0.059, -0.049],\n",
       "         [ 0.091, -0.380,  0.196, -0.430, -0.087,  0.072, -0.026, -0.357],\n",
       "         [-0.025,  0.216, -0.008,  0.256, -0.034, -0.077,  0.036, -0.075],\n",
       "         [ 0.003, -0.405, -0.497, -0.083, -0.313, -0.223, -0.567,  0.764],\n",
       "         [-0.146,  0.301, -0.082,  0.229,  0.245, -0.121,  0.440,  0.041],\n",
       "         [ 0.284, -0.499,  0.389, -0.451, -0.127, -0.653, -0.268,  0.411],\n",
       "         [-0.320, -0.114, -0.353,  0.160, -0.144,  0.482,  0.417,  0.028]],\n",
       "\n",
       "        [[ 0.017,  0.216, -0.166, -0.169,  0.083,  0.134, -0.030, -0.124],\n",
       "         [-0.086, -0.163,  0.471,  0.516, -0.016, -0.168,  0.360, -0.040],\n",
       "         [-0.013, -0.517,  0.140, -0.457,  0.242, -0.090, -0.049,  0.473],\n",
       "         [ 1.206, -0.802, -0.027, -0.124, -0.444, -0.151,  0.391, -0.148],\n",
       "         [-0.891,  0.947, -0.159, -0.076, -0.014,  0.050, -0.098,  0.310],\n",
       "         [-1.066,  0.817, -0.018,  0.348,  0.159,  0.127, -0.064,  0.207],\n",
       "         [ 0.406, -0.391, -0.191, -0.181, -0.060, -0.181,  0.051, -0.272],\n",
       "         [-0.560,  0.816, -0.118,  0.191, -0.237,  0.033,  0.190,  0.188]],\n",
       "\n",
       "        [[-0.008,  0.061,  0.172,  0.753,  0.517, -0.499, -0.309, -0.461],\n",
       "         [-0.507,  0.089,  0.580,  0.508, -0.075,  0.468, -0.550, -0.518],\n",
       "         [-0.460, -0.015,  0.146,  0.310,  0.088,  0.201, -0.462, -0.234],\n",
       "         [ 0.132,  0.366,  0.649,  0.135, -0.879,  0.475, -0.427, -0.377],\n",
       "         [ 0.411, -0.173, -0.309,  0.328,  0.102, -0.629,  0.239, -0.163],\n",
       "         [-0.476, -0.285, -0.028, -0.692, -0.201,  0.958, -0.052,  0.465],\n",
       "         [ 0.145, -0.310, -0.312, -0.531,  0.155, -0.256,  0.398,  0.527],\n",
       "         [-0.553, -0.481, -0.502,  0.285,  0.675, -0.784,  0.399, -0.059]],\n",
       "\n",
       "        [[ 0.119, -0.201,  0.162,  0.430,  0.255,  0.011,  0.083, -0.020],\n",
       "         [ 0.048,  0.016,  0.166, -0.416,  0.065,  0.311, -0.016,  0.109],\n",
       "         [-0.392,  0.012,  0.232, -0.085,  0.270, -0.158,  0.365,  0.131],\n",
       "         [ 0.355,  0.032, -0.126,  0.359,  0.205,  0.082, -0.035,  0.112],\n",
       "         [ 0.115, -0.200, -0.864, -0.327, -0.150, -0.056, -0.635, -0.181],\n",
       "         [ 0.108, -0.123, -0.272,  0.274, -0.013,  0.036, -0.126, -0.133],\n",
       "         [ 0.879,  0.073, -0.661,  0.117, -0.020,  0.361, -0.424,  0.112],\n",
       "         [-0.122,  0.108, -0.124, -0.315, -0.130, -0.300, -0.362,  0.008]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_printoptions(3,sci_mode=False)\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5# (4, 8, 16) @ (4, 16, 8) = (4, 8, 8) = (B, T, T)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ff974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.411, 0.589, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.366, 0.228, 0.406, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.217, 0.276, 0.220, 0.287, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.255, 0.170, 0.155, 0.234, 0.186, 0.000, 0.000, 0.000],\n",
       "         [0.132, 0.206, 0.141, 0.192, 0.195, 0.135, 0.000, 0.000],\n",
       "         [0.214, 0.098, 0.237, 0.103, 0.142, 0.084, 0.123, 0.000],\n",
       "         [0.085, 0.105, 0.082, 0.138, 0.102, 0.190, 0.178, 0.121]],\n",
       "\n",
       "        [[1.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.519, 0.481, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.361, 0.218, 0.421, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.592, 0.079, 0.172, 0.157, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.071, 0.448, 0.148, 0.161, 0.171, 0.000, 0.000, 0.000],\n",
       "         [0.047, 0.310, 0.134, 0.194, 0.160, 0.155, 0.000, 0.000],\n",
       "         [0.225, 0.101, 0.124, 0.125, 0.141, 0.125, 0.158, 0.000],\n",
       "         [0.062, 0.247, 0.097, 0.132, 0.086, 0.113, 0.132, 0.132]],\n",
       "\n",
       "        [[1.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.355, 0.645, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.228, 0.355, 0.417, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.202, 0.256, 0.339, 0.203, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.270, 0.151, 0.132, 0.249, 0.198, 0.000, 0.000, 0.000],\n",
       "         [0.099, 0.120, 0.155, 0.080, 0.130, 0.416, 0.000, 0.000],\n",
       "         [0.174, 0.110, 0.110, 0.089, 0.176, 0.117, 0.224, 0.000],\n",
       "         [0.072, 0.077, 0.076, 0.167, 0.246, 0.057, 0.187, 0.118]],\n",
       "\n",
       "        [[1.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.508, 0.492, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.229, 0.343, 0.428, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.299, 0.216, 0.185, 0.300, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.285, 0.208, 0.107, 0.183, 0.218, 0.000, 0.000, 0.000],\n",
       "         [0.183, 0.145, 0.125, 0.216, 0.162, 0.170, 0.000, 0.000],\n",
       "         [0.294, 0.131, 0.063, 0.137, 0.120, 0.175, 0.080, 0.000],\n",
       "         [0.128, 0.161, 0.127, 0.105, 0.127, 0.107, 0.100, 0.145]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=2)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "print(out.shape)\n",
    "\n",
    "wei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d096133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "context_size = 128 # what is the maximum context length for predictions?\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "n_emb = 384\n",
    "n_head = 6\n",
    "n_blocks = 4\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3216e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False) # (32, 8)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_size = head_size # 8\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # (64, 8, 32)\n",
    "\n",
    "        q = self.query(x) # (64, 8, 32) @ (32, 8) = (64, 8, 8)\n",
    "        k = self.key(x) # (64, 8, 8)\n",
    "        v = self.value(x) # (64, 8, 8)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * self.head_size ** -0.5 # (64, 8, 8) @ (64, 8, 8)\n",
    "\n",
    "        # wei - scaled dot-product attention matrix\n",
    "\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v # (64, 8, 8) @ (64, 8, 8)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_num, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(head_num)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(n_emb, n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_emb,n_emb),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.block(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.head_size = n_embd // n_head # 32 // 4 = 8\n",
    "        self.sa_heads = MultiHeadAttention(n_head, self.head_size)\n",
    "        self.feedforward = FeedForward()\n",
    "        self.norm1 = nn.LayerNorm(n_emb)\n",
    "        self.norm2 = nn.LayerNorm(n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.norm1(x))\n",
    "        x = x + self.feedforward(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_emb)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd=n_emb, n_head=n_head) for _ in range(n_blocks)])\n",
    "        self.ly_norm = nn.LayerNorm(n_emb)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "            \n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C) -broadcast-> (B, T, C)\n",
    "        # print(tok_emb.shape, pos_emb.shape)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ly_norm(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size )\n",
    "\n",
    "        # logits = self.block1(tok_emb)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:,-context_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc9ab01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of \n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4c2c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3568, val loss 4.3565\n",
      "step 100: train loss 2.6311, val loss 2.6414\n",
      "step 200: train loss 2.5080, val loss 2.5145\n",
      "step 300: train loss 2.4606, val loss 2.4724\n",
      "step 400: train loss 2.4152, val loss 2.4289\n",
      "step 500: train loss 2.3572, val loss 2.3766\n",
      "step 600: train loss 2.2901, val loss 2.3104\n",
      "step 700: train loss 2.2229, val loss 2.2527\n",
      "step 800: train loss 2.1689, val loss 2.1991\n",
      "step 900: train loss 2.1193, val loss 2.1576\n",
      "step 1000: train loss 2.0729, val loss 2.1226\n",
      "step 1100: train loss 2.0371, val loss 2.0899\n",
      "step 1200: train loss 2.0008, val loss 2.0640\n",
      "step 1300: train loss 1.9701, val loss 2.0403\n",
      "step 1400: train loss 1.9406, val loss 2.0178\n",
      "step 1500: train loss 1.9148, val loss 1.9990\n",
      "step 1600: train loss 1.8905, val loss 1.9821\n",
      "step 1700: train loss 1.8656, val loss 1.9630\n",
      "step 1800: train loss 1.8478, val loss 1.9539\n",
      "step 1900: train loss 1.8283, val loss 1.9435\n",
      "step 2000: train loss 1.8072, val loss 1.9280\n",
      "step 2100: train loss 1.8032, val loss 1.9159\n",
      "step 2200: train loss 1.7718, val loss 1.9041\n",
      "step 2300: train loss 1.7639, val loss 1.8913\n",
      "step 2400: train loss 1.7431, val loss 1.8848\n",
      "step 2500: train loss 1.7344, val loss 1.8808\n",
      "step 2600: train loss 1.7210, val loss 1.8663\n",
      "step 2700: train loss 1.7103, val loss 1.8647\n",
      "step 2800: train loss 1.6968, val loss 1.8566\n",
      "step 2900: train loss 1.6889, val loss 1.8475\n",
      "step 3000: train loss 1.6831, val loss 1.8338\n",
      "step 3100: train loss 1.6680, val loss 1.8252\n",
      "step 3200: train loss 1.6546, val loss 1.8141\n",
      "step 3300: train loss 1.6503, val loss 1.8090\n",
      "step 3400: train loss 1.6445, val loss 1.8089\n",
      "step 3500: train loss 1.6321, val loss 1.8027\n",
      "step 3600: train loss 1.6298, val loss 1.7975\n",
      "step 3700: train loss 1.6233, val loss 1.7934\n",
      "step 3800: train loss 1.6142, val loss 1.7865\n",
      "step 3900: train loss 1.6079, val loss 1.7841\n",
      "step 4000: train loss 1.6047, val loss 1.7745\n",
      "step 4100: train loss 1.6058, val loss 1.7639\n",
      "step 4200: train loss 1.5869, val loss 1.7670\n",
      "step 4300: train loss 1.5883, val loss 1.7593\n",
      "step 4400: train loss 1.5788, val loss 1.7467\n",
      "step 4500: train loss 1.5776, val loss 1.7438\n",
      "step 4600: train loss 1.5691, val loss 1.7467\n",
      "step 4700: train loss 1.5634, val loss 1.7400\n",
      "step 4800: train loss 1.5640, val loss 1.7392\n",
      "step 4900: train loss 1.5579, val loss 1.7386\n"
     ]
    }
   ],
   "source": [
    "max_iters = 5000\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7ebc169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sea shallow may not it yours, let his I madne\n",
      "mast is. 'Then is out\n",
      "Hatatch tedght for is twoo!\n",
      "\n",
      "Nurse: Pet is pate:\n",
      "No; thereat of I thatt thee on: say.\n",
      "\n",
      "Sirst My hout we mearth othis and sakely,\n",
      "And death, Poosting hast of all them my sapen.\n",
      "And biddeen'd marry by shall of comest incane.\n",
      "The sounter, it is ue.\n",
      "\n",
      "FRY:\n",
      "\n",
      "HENRY Who cangre?\n",
      "\n",
      "LICIO:\n",
      "Ast potse, all.\n",
      "\n",
      "Pardon:\n",
      "Nay, the sin my morere?\n",
      "\n",
      "Shour part, this mone malk thee did rempty fair,\n",
      "Is ip my from thas ey, tather father,\n",
      "The the know the vablicts? and all them wear,\n",
      "To dill:t if will that many your vanter\n",
      "I were porracts? O tricl'd me bearsediance do mone's all petruee,\n",
      "For that his maout: fought sir, friends, puright.\n",
      "I'll thee, do remouse.\n",
      "\n",
      "PAMHINDIUS:\n",
      "If smand there, is for natter too formicre\n",
      "As tho than batto serve-love! That the eive of olt.\n",
      "\n",
      "VIRCAHUM:\n",
      "What fair ham lord: thoub you would shall, Comeo?\n",
      "\n",
      "Some Woorten from'd: you wam bethan:\n",
      "What, I'll she siger, soild fladier, joy,\n",
      "Ound blawd's have to strief my dispan.\n",
      "\n",
      "Fi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "content = m.generate(context, max_new_tokens=1000)[0].tolist()\n",
    "print(\"\".join(decode(content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd1649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
